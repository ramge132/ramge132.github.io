---
title: Bootstrapping Language-Image Pre-training(BLIP, BLIP2) 논문 리뷰
layout: post
description: bootstrapping language and image pre-training
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/218607574-b22c5c47-40e7-475a-ace9-4d265c854d6a.gif
category: paper review
tags:
- VLP
- Bootstrapping
- Large language models
---

# BLIP을 제시한 이유

기존에 많이 살펴보았던 <U>CLIP</U>과 같이 vision과 language의 multimodality를 활용한 학습을 <U>Vision-Language Pre-training</U>(VLP)이라고 부른다. 그러나 기존 VLP의 학습 형태는 understanding based task(Encoding을 통해 downstream task를 해결하는 구조) 혹은 generation based task(Decoding을 통해 특정 modality를 생성하는 구조) 둘 중 하나만 잘 해결하는 경향이 있다. 게다가 VLP의 성능을 높이기 위해서 CLIP과 같은 연구에서 사용한 방법은 web으로부터 얻은 noisy한 image-text dataset을 활용하는 것이었는데(여기서 noisy란 뜻은 잘 정제된 데이터가 아니라, 무작정 웹으로부터 많이 수집하려다 보니 불필요한 데이터셋, 예를 들어 어떤 text prompt와 크게 관련없는 image 데이터가 섞여있는 경우를 의미함),  사실 이런 문제는 supervision을 보다 많이 활용할 수 있는 방법에 대한 차선책이지, 정말로 **이상적인 방법은 아니라는 것**이다. 가장 좋은 supervision을 줄 수 있는 방법은 당연하게도 web에서 얻은 데이터셋 크기와 같이 billion 단위의 pair를 구성하고, 각 pair가 완벽한 prompt matching이 되는 경우일 것이다. 하지만 이는 현실적으로 불가능하다.

BLIP paper에서 제시하고자 하는 것은 새로운 VLP framework를 제시하여 앞서 말했던 understanding task와 generation task 함께 학습할 수 있게끔 하는 것이다. 함께 성능을 높일 수 없었던 서로 다른 두 task를 통합함으로써  얻을 수 있는 장점은 multitask learning을 통해 보다 넓은 범위의 vision-language task(VL task) 를 수행할 수 있고, 각각의 성능을 향상시킬 수 있다는 점이다. 물론 단순히 여러 task에 대해 동시에 학습해서 성능 향상이 이루어진 것은 아니고 저자들이 제시한 것은 generation task에서 사용될 수 있는 **decoder**와 understanding task에서 사용될 수 있는 **encoder**를 일종의 caption synthesizer와 caption filter로 사용한 것이다. 이는 web에서 획득한 텍스트 기반 이미지 dataset의 noise를 줄일 수 있는 방법이 되었고, 결국 여러 task에 대해 성능을 올릴 수 있었다고 한다. BLIP 논문의 링크는 다음과 같다([논문 링크](https://arxiv.org/abs/2201.12086)).

---

# 기존 VLP의 한계점들

위에서도 짧게 언급했지만 기존 VLP 연구의 한계점은 크게 model의 관점에서와 data의 관점에서 설명해볼 수 있다.

### Model perspective

대부분의 VLP 구조는 encoder-based 아키텍쳐를 가지고 있거나([CLIP](https://arxiv.org/abs/2103.00020), [VILLA](https://arxiv.org/abs/2006.06195)) 보통의 transformer와 같이 encoder-decoder 아키텍쳐를 가진다([Unifying Vision and Language](https://arxiv.org/abs/2102.02779), [Deep Learning Library](https://arxiv.org/abs/1912.01703)). 그러나 encoder만 사용한 구조는 text generation task에 직접적으로 적용되기 힘든 구조를 가지고(실제로 representation 자체가 transfer되기 어렵다), encoder-decoder는 text 생성에만 집중하다보니 image-text retrieval같이 상호 modality간의 understanding이 필요한 task의 성능이 그리 좋지 않다는 것을 확인하였다.

### Data perspective

CLIP, [ALBEF](https://arxiv.org/abs/2107.07651)(about alignment) 그리고 [SimVLM](https://arxiv.org/abs/2108.10904)(about weakly supervised learning)과 같은 대부분의 SOTA 모델들은 Web으로부터 수집 가능한 대량의 image-text pair로 학습된다. 물론 dataset을 증가시킴으로써 얻을 수 있는 장점은 performance가 증가한다는 것이지만, 이 논문에서 밝힌 것은 단순히 vision-language learning에서 데이터셋 크기만 키우는 것이 정공법은 아니고, noisy web text(이미지와 상관 없는 텍스트)를 활용하는 것은 그리 좋지 않을 수 있다는 사실이다. 논문에서 이러한 한계를 뛰어넘을 수 있는 BLIP을 제시하였고, 모델 구조의 관점에서나 데이터 관점에서 모두 VLP framework에서 학습에 효율적으로 적용될 수 있는 방법을 제시하였다. 

---

# Contribution

논문에서 밝힌 data와 model 관점에서의 contribution은 다음과 같다.

### Multimodal mixture of Encoder-Decoder(MED)

보다 <U>유기적으로 transfer learning</U>을 진행할 수 있는 model architecture를 제시한다. MED는 단순히 encoder based나 encoder-decoder structure가 아닌 unimodal encoder, image-grounded text encoder 그리고 image-grounded text decoder를 아우른다. Model은 multitask learning에서 진행하는 것과 같이 각 구조별로 $3$개의 objective function으로 학습되는데, **unimodal encoder**에 대해서는 image embedding과 text embedding 간의 <U>contrastive learning</U>, **image-grounded text encoder**에서는 <U>image-text matching</U>을 최적화하게 되고 마지막으로 **decoder**에서는 <U>image-conditioned language modeling</U>을 최적화하게 된다.

### Captioning and Filtering(CapFilt)

Noisy한 image-text pair로부터 dataset을 bootstrapping할 수 있는 방법으로 제시된 것이 <U>synthetic model</U>(decoder)인 **captioner**와 <U>understanding model</U>(encoder)인 **filter**를 사용하여 original <U>web caption</U>과 <U>synthetic caption</U> 중 **noisy한 샘플을 없애는 작업**이다. 뒤에서 더 디테일하게 설명하겠지만 captioner와 filter는 서로 독립적으로 활용될 수 있으며, 단순히 synthetic caption을 만들지 않고 filtering만 진행하거나(filter only) captioning만 진행하는 경우(captioner only) 둘 다 사용하는 것보다 성능이 좋지 않은 것을 확인하였고, 이를 통해 성공적으로 caption을 bootstrapping할 수 있음을 확인하였다. 그리고 더 다양한 caption을 만들어내는 것이 augmentation 역할을 함으로써 성능 향상에 큰 기여를 했다고 한다.

위의 두 방법을 사용하여 만든 BLIP 모델은 image-text retrieval, image captioning 그리고 VQA(video question answering) 등등 다양한 task의 성능을 높일 수 있었다. 그리고 해당 모델을 단순히 video-language task에 transfer했을 때 zero-shot performance가 SOTA 네트워크를 뛰어넘을 정도로 representation 학습이 잘될 수 있음을 확인하였다.

---

# Related works

### Vision-language pre-training

VLP task에서 vision model과 language 모델을 학습함에 있어서 초점을 맞춘 것은 성능이 뛰어난 <U>human-annotated text를 구하기 힘든 이유</U> 때문에 web 상에서 크롤링한 image/text pair를 사용했다는 점이다. 그러나 대부분의 VLP task에서 사용한 web based dataset의 경우 filtering(유의미한 데이터셋을 잘 정제해내는 작업)을 간단하게만 진행하기 때문에 web text에 대한 <U>noise는 제거하지 못한 채</U> 사용할 수 밖에 없다. 물론 noise에 의한 drawback은 존재하지만, 그보다 <U>데이터셋을 증가시킴에 따른 성능 향상이나 장점</U>이 더 크기 때문에 이를 간과하고 넘어가게 된다. 이 논문에서는 기존 VLP 방식에서 짚고 넘어가지 않은 noisy web text를 다루면서, noisy web text를 데이터셋으로 사용할 수 밖에 없는 상황은 차선책인 것임을 보이며 CapFilt(Captioner and Filter)를 제안하였다.

기존에는 vision task와 language task를 통합하기 위한 다양한 방법론이나 연구들(image captioning, VQA 등등)이 제안되었지만 가장 해결하기 어려웠던 점은 <U>understanding task</U>와 <U>generative task</U> 성능에 모두 최적화가 가능한 구조를 찾기 힘들었던 것이다. 그간의 encoder-decoder model이나 encoder only 혹은 decoder only 구조와는 다르게 구조들을 modality의 관점으로 섞어서 만들게 된 BLIP 구조는 다양한 downstream task를 처리할 수 있으면서 동시에 pre-training 과정도 간단하게 구성한 연구라고 주장한다.

### Knowledge distillation

<U>Knowledge distillation</U>(KD)는 모델의 경량화와 함께 제안된 방법 중 하나이다. 가장 일반적인 knowledge distillation은 단일 task에 대해 좋은 성능을 보이는 teacher network와, 해당 teacher network의 representation을 배우고자 하는 student network를 포함하게 된다. 만약 classification task라면 student network가 모방하는 것은 teacher network가 예측한 각 class의 probability가 되며, 이때 최적화하는 것은 예측 확률에 대한 KL-divergence가 된다. 추가로 확장되어 나온 개념인 <U>self-distillation</U>이란 teacher network와 student network가 서로 같은 구조를 가진 경우를 의미한다. KD와 같은 방법론은 classification이나 VLP task에서 효과적이라는 연구들이 진행되어왔다. BLIP 논문에서도 마찬가지로 KD의 개념이 포함되는데, 기존 KD 연구에서와 같이 teacher network의 prediction을 student network가 완벽히 모방하는 형태가 아닌, <U>captioner</U>가 semantic-rich synthetic caption(이미지 정보를 풍부하게 담아낸 합성된 캡션)의 knowledge를 distill하게 되면, <U>filter</U>가 noisy caption을 걸러내면서 distillation하는 과정을 거치게 된다. 비유를 하자면 distillation 과정에서 편집이 진행되는, 즉 라이브 강의가 아닌 일종의 <U>잘 정제된 인터넷 강의</U>라고 볼 수 있으려나 싶다.

### Data augmentation

Computer vision에서의 data augmentation은 <U>dataset regularization</U>으로, overfitting을 방지하는 방법으로 널리 연구되어왔다.  그러나 language task에 대한 data augmentation은 domain specific(문법을 고려하거나, 언어 특성상 문맥이나 상황에 따라 묘사하는 범위가 크게 달라질 수 있음)하기 때문에 불명확한 점이 없지 않아 있었다. 최근에 들어와서는 language model 중 generative model을 토대로 synthesize하는 방법을 통해 augmentation을 진행하게도 했다. 기존 NLP에서 generative model을 사용하는 등의 연구들은 대부분 low-resource(언어 모델의 학습)과 관련된 task에 주로 적용되었지만, 이 논문에서는 <U>synthetic caption dataset</U>을 VLP에 사용했다는 점이 차이가 될 수 있다.

---

# Method

저자들이 제시한 BLIP의 구조는 VLP(Vision-Language Pre-training) 과정에 있어서 noisy-image pair로부터 학습할 수 있는 효율적인 방법을 제시하고, 그와 더불어 CapFilt라는 방식을 통해 논문의 가장 main contribution이라고 할 수 있는 <U>dataset의 bootstrapping</U>를 이끌어낸다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607874-ab1d0447-78ca-431f-b45f-9d74f7afdbbd.png" width="1000"/>
</p>

### Model architecture

**아키텍쳐**는 위의 그림을 보면 직관적으로 파악이 가능한데, 결론부터 말하자면 BLIP이 제시하는 trainining 구조는 <U>MED(Multimodal mixture of encoder-decoder)</U> 구조를 가지며, 총 3개의 역할을 달리하는 encoder 및 decoder 구조가 융합된 형태를 지닌다.

우선 CLIP과 같은 VLP 네트워크에서 학습하는 image와 text embedding 사이의 <U>contrastive learning</U>을 위해 unimodal encoder를 image domain과 text domain에 대해서 사용한다. **Image domain**에서의 encoder는 Vision transformer를 사용하고 **text encoder**는 BERT를 사용하게 된다. 이에 저자들은 특정 image를 기반으로 유의미한 caption을 필터링하기 위한 understanding module로 image-grounded text encoder를 사용하였다. Image-grounded text encoder는 위의 그림에서 중간에 cross-attention layer를 통해 image embedding 정보를 text encoder에서 활용함으로써 이미지와 텍스트의 matching 정도를 임베딩으로 내놓게 된다.

그리고 마지막으로는 image를 기반으로 synthesized caption을 augmentation(논문에서 사용한 용어를 토대로 정정하자면, dataset bootstrapping) 방법으로 사용하기 위해 설계한 text decoder가 존재하게 된다. 학습 framework에 대한 description은 아래에서 보다 자세히 다루도록 하겠지만, 우선 위의 그림에서 보이는 구조를 크게 파악하고 넘어가는 것이 논문을 이해하는데 큰 도움이 되는 것 같다.

### Pre-training objectives

**아키텍쳐 설명**과 더불어 figure에는 각 functionality를 담당하는 구조가 최적화할 objective function을 함께 명시해놓았다. 우선 가장 좌측부터 보면 <U>ITC(Image-text contrastive loss)</U>가 있는데, 이는 visual transformer와 text transformer가 서로 positive pair(image와 text description이 일치하는 경우)라면 가깝게, negative pair(일치하지 않는 모든 경우)라면 멀게 encoding하게끔 unimodal encoder를 학습하게 한다. Vision과 text에 대한 <U>understanding을 improving</U>할 수 있는 방법으로, 앞서 설명했던 바와 같이 대부분의 VLP에서 진행하는 contrastive learning 과정이라고 이해하면 된다.

두번째로 보이는 <U>ITM(Image-text matching loss)</U>는 image grounded text encoder를 학습하는 과정으로, binary classification task를 수행하게 된다. Encoder가 추출하는 prediction은 unimodal vision encoder에서 획득할 수 있는 semantic 정보를 text embedding과의 attention을 통해 주어진 text가 image에 positive(match)인지 negative(unmatch)인지 구분하게 된다. 그러나 단순히 BCE task로 접근하게 되면 contrastive loss와는 다르게 <U>negative sample의 어려움 정도</U>에 따라 loss optimization이 진행될 수 없기 때문에 <U>hard negative mining strategy</U>를 통해 분류가 어려운 negative sample이 학습에 더 많은 관여를 할 수 있게끔 해주었다.

마지막으로 <U>LM(Language modeling loss)</U>는 image grounded text decoder를 사용하게 되고, autoregressive한 방법으로 주어진 text input에 대해 cross-entropy loss를 최적화하게 된다. VLP에서 사용되는 MLM loss와는 다르게 LM loss는 visual information을 기반으로 유의미한 caption을 생성할 수 있게끔 학습 가능하다.

구조를 보게 되면 학습 시 필요한 네트워크 구조가 총 4개이므로 parameter 수가 급증하게 된다. 이를 방지하기 위해서 저자들은 self-attention layer를 제외하고는 text encoder와 text decoder의 모든 parameter를 공유하는 형태의 multi-task learning으로 접근했다. Self-attention layer를 제외한 이유는 encoding과 decoding task가 input에 있어 집중하는 부분이 서로  다르기 때문인데, 예를 들어 encoder는 bi-directional attention을 사용하지만 decoder는 causality가 보장되어야하기 때문(학습 과정에서는 현재 예측하는 토큰 이후의 context는 attention에 관여할 수 없음)이다. 이를 제외하고 image grounding 역할을 하는 cross-attention layer나 FFN network는 큰 차이가 없었기 때문에 두 네트워크의 parameter를 공유하는 방법을 통해 학습 효율을 높였다고 한다.

### CapFilt

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607877-6bb08a32-dd2b-48dd-826b-f5814d00f125.png" width="1000"/>
</p>

**Annotation cost** 때문에 **image-text pair**를 만드는 과정에서 web-crawled dataset$(I_w, T_w)$로는 high-quality pair$(I_h, T_h)$를 만들기 어렵다는 문제가 있었다. 대량의 데이터셋을 구축하는 과정에서 alt-text가 이미지를 제대로 묘사하지 못할 경우 <U>최적의 distribution과는 멀어질 가능성</U>이 있다. 이러한 noise 문제를 기존에는 <U>데이터셋의 크기에 의한 장점</U> 때문에 무시하고 있었으나, BLIP 논문에서는 위와 같은 CapFilt 구조를 통해 해결해보고자 하였다. 

데이터셋은 human annotated(high quality dataset)인 $(I_h, T_h)$ 그리고 $(I_w, T_w)$를 같이 사용한다. <U>High quality dataset</U>은 <U>web based dataset</U>과는 다르게 noisy alt-text가 없기 때문에 이를 filter와 captioner의 finetune 과정에 사용한다. Fine-tuned text decoder는 synthetic text $T_s$를 각 web image $I_w$에 대응하여 생성하게 되고 fine-tuned text encoder는 대응되는 이미지 $I_w$를 grounding으로 간주하여 $T_w$와 $T_s$를 필터링하게 된다. 만약 web dataset 중 $I_w$를 제대로 묘사하는 text prompt가 아닌 경우가 있다면 이를 제외시키고, text decoder에 의한 caption 또한 마찬가지의 과정을 거친다. 최종적으로 CapFilt를 통해 추출된 dataset은 필터링이 불필요한 high-quality dataset $(I_h, T_h)$, filtering된 web dataset $(I_w, T_w)$ 그리고 $(I_w, T_s)$이다. <U>이런 과정을 통해 bootstrapping된 dataset</U>을 이용하여 새로운 network를 pre-training하는데 사용했다고 한다. 이를 반복할수록 **더 높은 quality**의 dataset이 학습에 사용되고, 그로 인해 **네트워크의 성능**을 높일 수 있었다.

---

# Limitations and appearance of BLIP-2

위의 학습 과정을 보면 대략 알 수 있겠지만 <U>여전히 한계점이 존재</U>한다. 우선 pre-training 과정에서 parameter를 공유하는 방법을 통해 전체적으로 학습이 되는(fine-tuning) 네트워크를 <U>단순화했다는 점에서는 어느 정도 장점</U>이 있지만, 그럼에도 불구하고 여전히 end-to-end로 네트워크를 학습해야하는 방법론에서는 벗어나지 못했다. ‘Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation’이라는 제목에서 알 수 있듯이 이 논문에서 보여주고자 하는 주된 장점은 성능이 좋은 image encoder와 text encoder, 그리고 text decoder 전반을 융합하여 다양한 task에 높은 performance로 적용 가능한 구조를 만들고자 한 것이다. 그렇기 때문에 여전히 modality gap을 줄이기 위해 수행해야하는 학습 과정이 복잡하다.

BLIP 2번째 논문에서는 이러한 <U>기존 방식들의 한계점을 문제로 삼으며</U> VLP 문제에 접근한다.  

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607879-5e02144a-fbcb-4675-b4a4-b7ef6b84d617.png" width="800"/>
</p>

논문의 main figure는 위와 같다. 결국 BLIP 2의 framework는 위 그림으로 모두 설명이 가능할 정도로 간단하다는 것을 볼 수 있다. BLIP 논문이 총 4개의 transformer block 구조를 각각 optimize하는 형태라면, 이 process는 단순히 2 step의 pre-training 과정을 통해 vision과 language의 alignment가 가능하다고 주장한다. 자세한 방법은 논문의 개요를 간단하게 짚은 뒤 설명하도록 하겠다.

---

# VLP research의 방향성?

CLIP과 더불어 vision과 language를 함께 학습하고자 하는 task들이 무수히 많이 소개됨에 따라 여러 연구가 진행되었다. 그러나 대부분의 방법론의 경우 학습 과정에서 cost가 너무 많이 드는 과정을 제시한다는 점이 큰 문제가 되었다. Vision-language 연구는 vision과 language 사이에 있기 때문에 자연스럽게 각각의 unimodal 연구들(vision은 computer vision에서의 SOTA, language는 NLP에서의 SOTA)을 잘 융합하는 것에 초점을 맞추게 된다. 이 논문에서는 vision model과 language model 의 generic하고 compute-efficient한 방법을 제시하여, pre-trained vision model과 language model을 직접 건드리지 않고(학습 과정에서 guidance에는 사용되지만, frozen된 상태로 사용된다) bootstrapping할 수 있는 연산 효율적인 방법들에 집중하였다. LLM(Large language models)는 일종의 GPT3나 BERT같은 대형 언어 모델을 지칭하는데, 보통 language 네트워크는 generation이나 zero-shot transfer 성능이 상당히 높은 것으로 알려져있지만, vision task와 같이 다른 task에 적용될 때 학습된 representation을 잊는다던가(catastrophic forgetting)하는 문제가 발생하지 않도록 pre-trained representation을 잘 보존하는 것이 관건이다.

VLP에서 이러한 unimodal LLM의 representation을 잘 활용하기 위해서는 cross-modal alignment가 잘 이루어지는 것(vision representation과 language representation 사이의 적절한 매칭 관계)이 중요하다. Text embedding space를 하나의 위상 공간 $T$라 정의하고 마찬가지로 이에 대응되는 image embedding space $I$를 생각해보았을 때, 각 manifold가 $1$대 $1$ mapping이 되는 것이 가장 이상적이고 바람직한 형태이며, 더 나아가서 유사한 이미지끼리의 위상 관계가 유사한 텍스트끼리의 위상 관계에서도 유지되는 것이 가장 중요하다. 이렇게 서로 다른 modality를 가지는 데이터들의 embedding space간의 관계성을 찾고자 하는 것이 multimodal 연구의 alignment 항목이며, VLP에서도 이를 찾기 위한 연구가 진행되었다.

하지만 BERT나 GPT3와 같은 대용량 언어 모델은 image modality를 supervision으로 가지지 않기 때문에 representation을 유지하기 위해 단순히 freezing하여 사용하는 것은 최적화하기 어렵다는 문제가 생긴다. 결국 BLIP-2 이전에 제시된 논문인 [Frozen](https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)이나  [Flamingo](https://arxiv.org/abs/2204.14198)의 경우 image to text generation loss에 의존하였고, 이러한 방식은 modality gap인 alignment를 충분히 최적화하기엔 부족한 loss로 판단되었다.

저자들은 이러한 기존 방식들이 가져던 한계점을 극복하고자 frozen unimodal models인 image encoder, LLM을 활용할 수 있는 방법으로 modality 사이에 Q-former(Querying Transformer)를 제안하였으며, 이 구조는 image encoder에서 나온 embedding에서 LLM에 유용하게 사용될 수 있는 embedding을 추출해내고, 이를 통해 LLM 성능을 높이고자 한 것이다. 처음에는 방법론만 단순하게 보고는 하나의 위상에서 다른 위상으로 옮겨가는 과정을 보고 **복합적인 형태의 STN**이나 **transformer encoder**라고 보였다. 그러나 이런 insight보다는 VLP 연구에서 사용되었던 방법 중 cross-attention layer가 있는데, 이걸 **단일 module로 구현**하여 **scalability를 높일 수 있는 연구**로 방향을 잡았다고 보는게 더 적합하다고 생각된다.

Q-former는 앞의 image encoder나 뒤의 text decoder를 굳이 학습할 필요가 없다는 점에서 module과 관련된 모든 연구들의 장점(효율적, lightweight)을 가지고 있으며 그와 동시에 vision과 language 사이에 유의미한 modality relationship을 간단한 학습 구조로 찾을 수 있다는 것을 밝혀내었다.

---

# Contributions of BLIP-2

원본 논문인 BLIP과 살짝 다른 제목으로 BLIP-2는 본인들의 contribution에 대해 잘 보여준다고 생각한다. BLIP-2의 풀네임은 ‘Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models’ 이다.

- BLIP-2는 frozen pre-trained image and text network를 **효율적으로 사용**하면서 Q-former를 통해 **modality gap을 줄일 수 있는 방법**을 제시하였다. 학습 방법은 representation learning stage와 text generative learning stage로 구분된다. BLIP-2는 **기존 방식들에 비해 간단한 구조**를 토대로 다양한 VL task에서 SOTA를 기록하였다.
- OPT, FlanT5와 같은 성능 좋은 LLMs을 기반으로 BLIP-2는 zero-shot image to text generation을 진행한다. 보다 좋은 LLM을 사용하여 좋은 visual knowledge reasoning, visual conversation이 진행될 수 있다는 것은 앞으로도 BLIP-2가 제안한 모듈을 통해 확장성 있는 접근이 가능하고, 이를 통해 vision language task의 성능을 높일 수 있다는 가능성이 있다. **즉, scalable한 approach**이다.
- Frozen unimodal models를 사용하고 lightweight Q-former를 학습함으로써 BLIP-2는 compute-efficient하게 성능을 높일 수 있었다. 예를 들어 BLIP-2는 앞서 소개했던 Flamingo보다 zero-shot VQA 성능이 $8.7\%$가 높았으며, 심지어 **54배 적은 parameter 수로 이를 충족**할 수 있었다.

---

# Related work

### End-to-end VLP(Vision-Language Pre-training)

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607881-56fa1d22-c69e-40ca-8c96-c9b906771fee.png" width="800"/>
</p>

위의 그림은 multimodal learning with transformers란 survey 논문([참고 링크](https://arxiv.org/abs/2206.06488))에서 밝힌 최근 multimodal learning research architecture의 구조들을 보여준다. Survey 논문에서도 밝히듯이, 각 task에 따라 가장 효율적인 아키텍쳐는 다르다. 즉 구조가 따로 정해져있지 않다는 것이 어쩌면 연구의 자유도를 높일 수 있는 장치이면서도 많은 ablation을 요구하는 challenging task에 해당된다. 네트워크 구조와 더불어 여러 pre-training objective function들도 함께 제시되었지만, 최근에는 대부분 image-text contrastive learning, image-text matching 혹은 masked language modeling과 같은 몇 개의 loss term이 주로 사용되었다.

여러 방법을 막론하고 대부분의 VLP 방법들은 **large scale(web based) image/text pair** dataset을 활용하여 end-to-end 학습을 진행한다. 최근 대용량 언어 모델이나 transformer based 아키텍쳐는 대부분 기존 CNN based architecture에 비해 성능 향상을 얻은 대신 그만큼의 trade-off로 parameter의 손실을 감수해야했고, 이에 따라 VLP 연구에서도 같은 bottleneck이 적용될 수밖에 없다. 게다가 이미 잘 학습된 representation을 가지는 LLMs과 같은 모델들의 representation을 효율적으로 leverage할 수 있는 방법을 찾기도 어려운 마당이다.

### Modular Vision-Language Pre-training(VLP)

앞서 언급한 방법들의 경우 pre-trained model을 frozen하는 방법을 사용하지 않고 학습 과정에서 이용하고자 하는 목적이 강했는데, 저자들은 이러한 방식을 on-the shelf라고 언급하면서 본인들은 SOTA 성능을 보이는 다양한 network들을 직접적으로 사용하는 것에서 벗어나 <U>off-the-shelf method</U>를 고안하기 시작했다고 한다. 그래서 그런지 앞서 언급한 related works보다는 <U>pre-trained network를 frozen</U>하는 방식이 이 논문의 ideation에 도움이 되었다고 할 수 있다.

초기 작업에서는 frozen object detector를 사용하여 visual feature를 뽑는 방법을 사용했으며 최근 [LiT](https://arxiv.org/abs/2111.07991)에서는 pre-trained image encoder for CLIP을 frozen 상태로 사용하였다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607884-2515bfe6-eb75-4b77-94dc-954117489268.png" width="700"/>
</p>

해당 논문에서 제안한 방법론에서는 <U>image encoder를 frozen</U> 상태로, <U>text encoder를 fine-tuning</U>하는 과정이 가장 성능이 좋게 나왔다고 한다. 아무튼 말하고자 하는 것은 CLIP representation을 유지하는 방향으로 학습 방법론을 제시하는 것이 도움이 된다는 연구 결과가 있었다는 점이다. LiT가 image encoder를 lock(frozen) 상태로 이용했다면 반대로 language model(LLM)을 freeze해서 사용하는 vision to language task도 소개되었다. Frozen LLM을 사용할 때 가장 어려운 점이 바로 text와 image semantic information 사이의 alignment인데, 이를 해결하기 위해 Frozen 논문에서는 LLM의 모델에 입력으로 들어가는 text prompt에 image encoding 결과를 soft prompt로 사용하였다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607886-a2c36b2f-baaa-4cb2-895e-f7ac345907a7.png" width="700"/>
</p>

LLM은 학습이 안되는 상태로, <U>vision encoder만 LLM의 도움을 받아</U> alignment를 진행하는 쪽으로 fine-tuning을 할 수 있다는 형태의 연구가 되었다. Flamingo는 Frozen 논문과는 다르게 input에서 alignment를 진행하지 않고 <U>feature map에서 alignment</U>를 진행했는데, <U>cross-attention layers</U>를 LLM에 추가함으로써 visual feature와 함께 text prompt가 attention을 진행할 수 있게 하였는데, 이때 새롭게 추가된 layer만 pre-training 시키는 방법을 사용한다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607888-cfc642a7-a09d-47e9-aaac-46363220b585.png" width="800"/>
</p>

두 방법 모두 공통점이라면 LLM 네트워크를 freeze한 상태로 사용한다는 점이고, 차이점은 **Frozen**은 image encoder를 fine-tuning하는 형태로 input alignment를 학습 목적으로 삼았고 **Flamingo**는 image encoder도 freeze한 상태로 cross-attention이 진행되는 일부 layer만 scratch부터 학습하였다는 점이다.

이러한 기존 방식들은 모두 image encoder나 LLM의 구조를 직접적으로 바꾸지는 않지만 <U>fine tuning하거나 레이어를 추가하는 식으로 alignment를 해결</U>하고자 하였고, 학습 과정에서 computational cost가 어느 정도 크다는 단점이 있다. 저자들은 이러한 문제들을 해결하기 위해 <U>BLIP-2 아키텍쳐를 제시</U>하였고, image encoder와 LLM의 representation을 <U>효율적으로 leverage</U>할 수 있는 방법을 제시하였다.

---

# Method

앞서 본 BLIP 논문에 비해 BLIP-2가 가지는 장점 중 하나는 메소드가 간단하다는 것이다. BLIP 논문에서는 학습을 위해 크게 $3$개의 부분으로 나뉘는 파트(unimodal representation learning, image-grounded matching and image-grounded text generation)가 각각 최적화되어야 했지만 이번 논문에서는 모듈을 제시하여 bootstrapping하는 방법을 사용하였다.

Vision modality와 language modality 사이의 modality gap을 bridge하는 모듈의 이름을 <U>Q-former(Querrying Transformer)</U>로 정의하였으며 Q-former에 의한 학습은 image encoder를 활용한 <U>vision-language representation learning stage</U>와 뒤이어 진행되는 LLM 기반 <U>generative learning stage</U>로 나뉜다. 그나마 limitation이라고 하면 단계별로 나뉜 training step이라고 할 수 있겠다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607891-a876e26d-6fa0-4f2f-b95c-e0060e53cc42.png" width="1200"/>
</p>

모델의 framework를 보여주는 figure는 위와 같다. **Q-former**는 <U>학습이 가능한 querry</U>를 가지고 있으며, 이 querry들은 이미지에서 특정 text가 주어졌을때 alignement에 활용될 수 있는 feature를 추출해주는 역할을 한다. Q-former가 없었던 BLIP 논문에서는 image-text matching, image-text contrastive learning 그리고 image-grounded text generation 모두 서로 다른 loss term으로 서로 다른 network(물론 text encoder와 decoder가 같은 weight으로 학습되었지만)를 최적화하는 방식이었지만, BLIP-2에서는 Q-Former의 encoder-decoder 구조에 대해서 <U>causality만 조절</U>해줌으로써 encoder에서와 decoder에서 각각의 task에 맞는 optimization이 진행될 수 있다는 장점이 있다. 실선으로 표시된 부분이 image and text matching(bi-directional)과 image-grounded text generation(uni-direction)을 보여주며 점선과 ‘x’ 표시로 이어진 부분이 각 uni-modal(encoder와 decoder) 부분의 contrastive learning이다. 우측 그림을 보면 알겠지만 image and text matching은 모든 semantic 정보를 참고하여 alignment를 진행할 수 있고, image-grounded text generation은 autoregressive한 생성 과정 때문에 causality가 학습 과정에 적용되는 것을 확인할 수 있다. 마찬가지로 image-text contrastive learning에서는 각 query token에 대한 positive sample(하얗게 표시된 부분)과 negative sample(회색으로 표시된 부분)에 대해 학습되는 것을 시각화하였다.

### Bootstrap vision-language representation learning from a frozen image encoder

Representation learning stage에서는 Q-former를 frozen image encoder와 연결하여 image-text pair를 통해 학습하게 된다. 처음 학습 단계에서 얻고자 하는 것은 Q-former가 가지는 학습 가능한 parameter인 querry를 잘 학습하여 visual representation으로부터 text prompt와 유의미한 관계를 가지는 특징을 추출하고자 하는 것이다. BLIP에서 얻은 아이디어를 바탕으로 세 개의 서로 다른 objective를 최적화하였다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607893-960183d0-a794-4292-9710-dcbade846a95.png" width="600"/>
</p>

Loss term에 대한 추가 ablation이 없었다는 것은 아쉽지만 우선 위의 표를 보게되면 ITG(generation) loss term이 미약하게나마 성능을 높인 것을 확인할 수 있다. 사실상 training 과정에서 첫번째 단계의 가장 중요한 점이 frozen image encoder가 보다 text prompt와 유의미한 관계를 가지는 feature를 뽑는 과정이기 때문에 **text generation loss(ITG)**가 성능에 주된 영향을 끼칠 것이라고 생각했는데 의외로(?) ITC+ITM loss가 성능 부분에 있어서 높은 결과를 보여주었다.

그리고 앞서 BLIP 논문을 설명할 때 빠진 부분이 있는데, negative sampling을 진행할 때 momentum queue를 사용했었는데 BLIP-2 paper에서는 batch 내부에서 positive pair와 negative pair를 구분짓는 방식을 사용했다고 한다. momentum queue에 대한 내용은 moco 논문들([참고 링크](https://arxiv.org/abs/1911.05722))과 SimCLR 논문([참고 링크](https://arxiv.org/abs/2002.05709))을 보면 보다 이해가 쉬울 것이다.

### Bootstrap vision to language generative learning from a frozen LLM

앞선 학습 과정을 통해 frozen image encoder와 Q-former 사이의 representation alignment이 마무리가 되었기 때문에 다음 작업으로 Q-former와 frozen LLM 사이의 alignment를 진행하게 된다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607895-2ea24f75-a584-4f40-b676-2b903d921e8c.png" width="1000"/>
</p>

앞서 학습 과정을 보면 Q-former 자체도 하나의 lightweight transformer이기 때문에 visual 정보에 대한 embedding을 output으로 내보내게 되는데, 이를 어쩐 일인지 그대로 LLM의 input으로 사용하지 않고 FC layer를 통해 projection을 해서 학습에 사용하였다.  아무래도 text prompt에 prepend(앞쪽에 붙여서) 학습한 것을 보니 input에 대해 alignment를 진행하는 frozen 방식과 유사하되 이때의 image encoder는 Q-former로 대신 생각하면 될 것 같다. ~~근데 fully connect layer를 곁들인..~~ 혹시나 해서 Frozen 논문을 찾아봤는데 해당 논문에서는 input에 prepend를 하는 과정에서 FC layer를 사용하지 않았다.

그리고 위의 그림을 보면 알 수 있지만 LLM을 사용할 때 decoder base랑 encoder-decoder base 모두 실험해보았다고 했다. Decoder base(OPT)에서는 FC layer로 나온 soft visual prompt를 그대로 output text 추출에 대한 input으로 사용하고, encoder-decoder base(FlanT5)에서는 FC layer로 나온 soft visual prompt를 원래의 prefix text에 붙여서 사용하게 된다.

---

# Results

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607897-2809d23c-ebdc-4767-b33b-972f384e0ed7.png" width="1000"/>
</p>
성능은 open-source가 가능한 BLIP과 비교했을 때도 상당히 높은 수치를 보여주었고 비교적 최근 SOTA인 Flamingo나 BEIT-3과 같은 구조에 대해서도 최대 $+8$의 정량적 평가 향상을 보여주며 그 성능을 보여주었다. Q-former의 학습은 representation alignment 과정이 주요하게 작용하는데, 이를 보여주는 것이 바로 다음과 같이 학습 stage를 구분했을 때와 아닐 때의 학습 차이를 보여주는 그래프이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607902-cc4d0603-2f7b-4f50-b83b-4cc506b75180.png" width="800"/>
</p>

그리고 학습 두번째 stage에서 학습에 쓸 수 있는 LLM baseline으로는 decoder based/encoder and decoder based 모두 가능하기 때문에 FlanT5와 같은 encoder-decoder 기반의 구조에서는 아래와 같이 image question and answering이 가능한 것을 볼 수 있다. 사실 아래 task는 VQA와 같은 question answering이라 불리지 않고 instructed zero-shot image-to-text generation이라고 부르는데 큰 차이는 없는 것 같다(아닌가?).

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/218607906-4e28961b-3d6b-4fdb-8aa3-bba81aa05179.png" width="800"/>
    <img src="https://user-images.githubusercontent.com/79881119/218607912-7c0b577d-aace-499c-90f0-9244361d1c6d.png" width="500"/>
</p>

그리고 우측 그림은 VQA fine-tuning에서 사용한 구조라고 한다.

---

# Conclusion and discussion

VLP task에서 보다 효율적으로 vision model과 language model을 함께 사용할 수 있는 방법을 연구한 BLIP과 후속 논문인 BLIP-2에 대해서 리뷰해보았다. 아카이브에 올라온 지 얼마 되지 않은 따끈따끈한 작품이지만 굉장히 흥미롭게 볼 수 있었고 이해를 완벽하게 하진 못한 부분들도 있었지만 insight를 얻어가기 괜찮은 논문이라고 생각했다.

하지만 의문이 들었던 점은 BLIP-2의 경우 FC layer를 사용했던 이유라던가, ITG loss가 직접적으로 performance에 그리 큰 영향을 끼치지 않는 것으로 보이는 부분에 대한 분석이었고, 그 이외에는 논문에서 자체적으로 저자들이 제시한 limitation인 few-shot에 대한 in-context learning 방법을 찾지 못했다는 점과 LLM의 up-to-date information의 부재로 인한 image to text generation의 failure case가 존재한다는 점이다.

Web을 기반으로 학습된 좋은 성능의 모델을 그대로 사용할 수 있다는 점은 장점이 되지만 LLM 네트워크를 학습할 때 존재하지 않았던 정보에 대한 inference를 image가 포함한다면 성능이 좋지 않을 수 있음을 보았다.