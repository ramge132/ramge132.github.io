---
title: cs231n 내용 요약 (5) - Neural Network
layout: post
description: Lecture summary
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210934049-762c2540-c097-4107-89ba-32bd2b8bc9f3.png
category: deep learning
tags:
- AI
- Deep learning
- cs231n
---

# 들어가며...

딥러닝의 침체기에 걸쳐있던 perceptron과 이를 극복하기 위한 multilayer perceptron 구조, 그리고 해당 구조를 학습시킬 수 있는 방법으로 backpropagation에 대해 알아볼 수 있었다. 이번 글도 똑같은 구조인 neural network에 대해서 살펴볼텐데, 이전까지는 방법론에 대한 내용이 주를 이루었다면 이번에는 그보다는 조금 더 <U>architecture, structure</U>에 대해서 살펴볼 예정이다.

---

# Neural network
뇌과학이나 해부학적인 지식이 없이도 neural network를 이해할 수 있다. 이는 perceptron의 설계가 직관적이고 수학적이기 때문이다. Linear classification에서는 주어진 이미지 데이터에 대해 <U>class별 점수를 계산</U>하고(score function $f$), 여기서 단일 perceptron의 score function을 구성하는 weight parameter $W$를 정의할 수 있었다. 예를 들어 CIFAR-10 dataset에서는 input으로 사용되는 벡터의 크기가 $32 \times 32 \times 3$의 column vector였으며, score function이 동작하게끔 weight는 $10 \times 3072$의 matrix로 구성하여 10개의 class에 대한 점수를 계산했었다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/212606281-bb0f97b8-8f89-4378-8f6b-6fde347130a7.png" width="700"/>
</p>

이는 단일 parameter에 대한 경우였고, 조금 더 복잡한 구조를 가진 neural network를 생각해보자. 가령 두 개의 perceptron이 서로 연결되어있는 형태의 score function이 다음과 같다고 생각해보면,

\[
    s = W_2 \max(0,~W_1x)    
\]

더이상 이 연산 결과는 affine하지 않게 된다. 여기서 affine하지 않다는 것은 **'선형적이지 않다'**는 뜻이다. 만약 $W_1$이 $100 \times 3072$의 matrix라면, $W1 \cdot x$ 연산을 통해 $100$ 차원의 hidden feature(vector)를 계산할 수 있다. $W_2$와의 연산 이전에 계산되는 $\max$ 함수의 경우 생성된 $100$차원의 vector에 대해 non-linear한 연산을 진행한다. Non-linear 연산을 진행하는 이러한 함수를 <U>activation function</U>이라고 부르며, 흔히 여러 개의 perceptron이 연결된 신경망 구조를 포함한 다양한 neural network 구조에서 연산의 노드를 분리하기 위한 장치로 사용된다. Activation function의 종류는 다양하며, 그 중 기본적인 형태에 대해서는 뒤에서 소개하도록 하겠다. 위에서 볼 수 있는 $\max$ 함수는 $0$보다 작은 값은 $0$으로, 그보다 큰 값은 그대로 내보내는 함수로 <U>Rectified Linear Unit</U>(ReLU) 함수라고 부른다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213332542-a5eb034c-356b-4eb0-8847-4500c1b3dcdc.png" width="700"/>
</p>

단순히 여러 층을 쌓는 neural network 구조를 생각해보자. 만약 linear operation을 $n$개의 layer에 대해 수행하게 되면,

\[
    y = x \bigodot_{i=1}^n W_n = x \odot W
\]

위와 같이 $\bigodot_{i=1}^n W_n = W$를 연산한 것과 동치가 되기 때문에 linear operation $1$개를 적용한 것과 차이가 없다. 즉 단순히 weight 개수만 늘리게 되면 <U>신경망 네트워크의 연산 복잡도</U>를 높일 수 없다는 것이다. Deep neural network를 구성하기 위해서는 activation function이 필수적임을 알 수 있다. 그리고 이렇게 연산되는 전체 네트워크를 학습하는 과정은 backpropagation임을 이전 글에서 확인하였다.

---

# Modeling neuron

신경망 네트워크의 시작은 <U>생체의 neural system</U>을 컴퓨터 환경에서 구현하는 것을 목표로 시작되었다. 결국 engineering 환경에서 성능을 좋게 만드는 것이 궁극적인 목표가 되겠지만, 신체의 neural system을 간단하게나마 컴퓨팅 환경이나 논리 환경에서 구현하는 것으로 연구가 시작되었다고 볼 수 있다. 우리 뇌가 계산을 하는 과정의 가장 기본이 되는 단위는 neuron(뉴런)이다. 대략 $860$억 개의 뉴런이 실제 인간의 뇌 신경 환경에서 쓰이고, 각각은 대략적으로 $10^{14}$ ~ $10^{15}$개의 시냅스로 서로 연결되어있다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213333618-16ab5dc1-3297-455a-a21d-6551a720bcf5.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213333676-5d89f078-3a26-403d-a40b-bd347794c813.png" width="300"/>
</p>

생물 시간은 아니지만 인체 신경망 구조가 작동하는 원리를 간단하게 보면 다음과 같다. 우선 각 뉴런 단위들은 input 신호를 **dendrites**를 통해 받아들이고, **axon**을 통해 output signal을 출력하게 된다. 각 **branch**를 따라서 axon들은 시냅스를 통해 다시 다른 뉴런들의 **dendrites**로 연결된다. 마찬가지로 이를 컴퓨팅 환경에서 구현한 perceptron에서는 신호들이 axon을 통해 전달되는 형태는 $x$라는 input으로 들어오는 형태로 구현이 되었으며, 그림에서 볼 수 있듯이 $n$-dimensional data의 각 dimension에 대해 서로 다른 시냅스를 통해 dendrites와 연결되게끔 한다. 이때 여기서 곱해지는 weight $W$는 각 시냅스에서 들어온 신호와 뉴런 각 dendrites에서의 <U>상호작용하는 정도</U>(strength)를 나타낸다. 일반화된 위의 그림과 같은 네트워크에서는 이렇게 각 dendrites로 받아들이는 신호들을 weighted summation하게 된다. Input에 대한 output이 단일 스칼라 value로 나오는 경우를 생각해볼 수 있다($n$ to $1$ function).

\[
    \sum_{i} w_i x_i + b
\]

만약 최종적으로 받아들인 모든 $W,~x$의 상호작용이 합해진 결과가 일정 threshold를 넘어서게 되면, 뉴런이 fire(activate)하여 활성화되고, 다시 다른 axon을 통해 신호를 전달하게 된다. 인간의 경우 시냅스에서 신호가 전달되고, 각 뉴런이 activate하는 타이밍이 전체 생체 활동에 중요한 부분을 차지하지만 computational 환경에서는 이를 가장 기본적인 형태로 <U>동기화시키기 때문에</U> spike 타이밍은 중요하지 않다. 그렇다면 생체 뉴런이 아닌 perceptron에서 고려해야할 것은 neuron이 얼만큼 들어온 신호에 대해서 **'fire(activation)'**하는지가 되는데 이를 'firing rate of the neuron'이라고 표현하며 이를 함수로 구현한 것이 곧 function $f$, <U>activation function</U>(활성화 함수)라고 부른다.

\[
    y = f \left( \sum_{i} w_i x_i + b \right)
\]

따라서 인간의 <U>생체 뉴런</U>에서의 **firing, activating** 과정이 <U>computer perceptron</U>에서 구현된 형태가 앞서 언급했던 non-linearity 요소를 줄 수 있었던 **activation function**이다. 지금까지 설명한 퍼셉트론을 실제 파이썬 환경에서 하나의 class로 선언하면 다음과 같다. ```forward``` 메소드가 곧 뉴런으로 하여금 ```inputs```($x$)에 연산을 진행하여 원하는 firing rate($0 \sim 1$)를 내보내게 된다. 여기서 firing rate이 <U>normalize</U>되는 효과가 생기는데, 이는 뒤에서 추가로 설명하도록 하겠다.

```python
class Neuron(object):
    def forward(self, inputs):
        """ assume inputs and weights are 1-D numpy arrays and bias is a number """
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
        return firing_rate
```

---

# Single neuron as a linear classifier

단일 뉴런이 표현하는 식은 앞선 예제에서 $\sigma \left( \sum_i w_i x_i + b \right)$으로 정의할 수 있었고, 이는 결국 주어진 weight $W$와 input $x$에 대한 class probability와 같다. 만약 이 문제에 대해서 두 개의 class가 있는 경우라고 생각해보자($y$는 $0$ 또는 $1$). 즉 $0 \sim 1$로 normalize된 하나의 값이 해당 class일 확률로 해석 가능하다는 것이다. 따라서 input $x$에 대하여 class가 $1$이 될 확률이 $p(y_i = 1 \vert x_i;~w)$이며, 반대로 class $0$이 될 확률이 $1 - p(y_i = 1 \vert x_i;~w) = p(y_i = 0 \vert x_i;~w)$가 된다.   
직관적으로 보면 위와 같았고 실제로 예시를 통해 확인해보면 다음과 같다. 결국 말하고자 하는 것은 <U>단일 perceptron</U>이 classification에서 사용되는 것을 통해 앞서 미리 살펴본 <U>softmax</U>를 활용한 <U>linear classification</U>과 연결짓는 것이다. 만약 특정 input $x_i ~ (i = 1,~2,~\cdots,~n)$에 대해서,

\[
    \sigma \left( \sum_i w_i x_i + b \right) = 0.2
\]

라고 가정해보자. Sigmoid 함수인 $\sigma$는 설명했던 것과 같이 input이 어떤 값이 되던 상관없이 $0 \sim 1$로 정규화가 가능하다고 말했었다. 따라서 위의 결과값을 토대로 $0$ 또는 $1$일 확률을 나타낸다면, 단순하게 다음과 같이 표현이 가능하다.

\[
    \sigma \left( \sum_i w_i x_i + b \right) = p(y_i = 1 \vert x_i;~w) = 0.2
\]

출력값이 $0$에 가까워지면 $1$일 확률이 줄어들고, $1$에 가까워지면 반대로 $1$일 확률이 늘어나는 구조로 해석 가능하다. 반대로 $0$일 확률은,

\[
    1 - \sigma \left( \sum_i w_i x_i + b \right) = 1 - p(y_i = 1 \vert x_i;~w) = 0.8
\]

이처럼 표현된다. 위의 상황은 softmax를 사용한 linear classification이었기 때문에 만약 SVM(Support Vector Machine)을 사용하게 된다면 activation function의 형태나 뉴런이 달라지게된다. 

\[
    R(W) = \sum_k \sum\_l W\_{k,~l}^2    
\]

Linear classification 글에서 소개했던 내용 중에서 data loss 외에 위의 식과 같은 <U>regularization loss</U>가 있다. 이런 regularization loss를 생체 뉴런으로 해석한 것이 'gradual forgetting'이다. 퍼셉트론을 구성하는 parameter가 최적화되는 과정에서 시냅스 가중치에 해당하는 weight를 $0$으로 유도하는 효과가 있기 때문이다.

​---

# Activation functions

인체 신경망 구조를 묘사한 퍼셉트론에서 firing rate을 구현하기 위한 구조적 장치로 activation function, non-linear function을 소개하였다. Neural network가 발전하면서 소개된 다양한 activation function를 확인해보자.

## Sigmoid($\sigma$) function
<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213339599-ab505b03-2644-4c3b-af92-c74a3c5a7aa2.png" width="400"/>
</p>

우선 가장 먼저 소개할 함수는 sigmoid 함수이다. 이 함수가 가장 역사적으로 유명해진 이유는 다음과 같은 특성이 고려되었기 때문이다. 일단 input으로 모든 실수를 input으로 받을 수 있으며(정의역이 실수 전체), neuron의 firing rate를 반영하듯 아주 작은 negative number에서는 $0$, 큰 positive number에서는 $1$이 mapping될 수 있기 때문이다. 하지만 sigmoid 함수는 다음과 같은 단점들을 가지고 있다.   
첫번째 문제는 Sigmoid 함수는 $x$가 너무 작거나 클 경우 <U>수렴하는 형태의 그래프</U>를 가지기 때문에, <U>gradient vanishing</U>이 발생한다(gradient가 0에 수렴하는 것)는 것이다. Backpropagation 과정을 recall하면 각 <U>local gradient</U>를 gate의 output으로 전달된 <U>이전의 gradient</U>에 곱하면서 네트워크 전체를 학습하게 되는데, 만약 <U>sigmoid gate</U>의 local gradient가 $0$에 수렴하면, **해당 gate 이전에 놓인 weight** 전체의 학습이 어려워지게 되고, weight initialization이 잘못된 채로 학습을 시작하면 처음부터 학습이 거의 안되는 불상사가 발생할 수도 있다.   
두번째 문제로는 sigmoid 함수는 중간값이 $0$이 아님을 들 수 있다. 만약 activation function의 value가 음수를 가질 수 없다면, parameter update 과정에서 모든 weight가 같은 방향으로 update가 된다는 점 때문에 학습 속도가 매우 저하될 수 있다. 예시를 통해 보면 다음과 같다. 만약 어떤 input($x$)이 neuron으로 들어올 때, input($x$)을 구성하는 모든 값이 positive하다고 가정해보자.

\[
    f = \sigma \left( W^\top x + b \right)  = \sigma \left( y \right)  
\]

그리고 sigmoid function에 대한 local gradient는 다음과 같다.

\[
    \frac{d \sigma(y)}{dy} = \left( \frac{1+e^{-y}-1}{1+e^{-y}} \right)\left( \frac{1}{1+e^{-y}} \right) = (1-\sigma(y))\sigma(y)  
\]

여기서 알 수 있는 사실은 sigmoid function의 output이 $0 \sim 1$ 사이의 값을 가지기 때문에, sigmoid gate를 통해 전달되는 gradient는 <U>모든 input에 대해 양숫값</U>을 가지게 된다. 따라서 chain rule을 통해 weight $W$에 대해서,

\[
    \begin{aligned}
        \frac{\partial f}{\partial W} =& \frac{\partial f}{\partial \sigma} \cdot \frac{\partial \sigma}{\partial y} \cdot \frac{\partial y}{\partial W} \newline
        =& (1-\sigma)\sigma x > 0
    \end{aligned}
\]

위와 같이 정리된다. 따라서 <U>모든 gradient가 동일한 방향</U>을 가지게 되므로 학습이 어렵다는 문제가 생긴다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213344264-7bf34af9-fd9b-4180-92eb-e7e5ebe6fdf9.png" width="300"/>
    <img src="https://user-images.githubusercontent.com/79881119/213344283-67231bd0-8f44-4329-b261-175a4ccffd55.png" width="300"/>
</p>

위의 그림이 weight column을 axis로 표현한 좌표계에서의 parameter update 과정을 나타낸 것이다. 만약 weight가 같은 방향으로만 update가 되어야한다면, 좌측과 같이 $W_2$에 대해서는 감소하면서 $W_1$에 대해서는 증가하는 형태로 학습이 불가능하므로, 우측과 같은 형태로 학습이 진행된다. 

## Hyperbolic tangent($\tanh$) function

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213344841-bc551100-7760-4ee9-b226-a50cbb320776.png" width="400"/>
</p>

위의 함수는 $\tanh(x) = 2\sigma(2x) - 1$로 표현되는 하이퍼볼릭 탄젠트 함수이다. 이 함수의 구조를 보게 되면 local gradient가
\[
    \frac{d}{dx} \tanh(x) = (1 - \tanh(x))(1 + \tanh(x))
\]

위와 같이 나와서 우선적으로 앞서 소개했던 sigmoid에 비해 gradient가 크다는 장점이 있다. 또한 sigmoid value($\sigma$)가 양수여서 <U>weight parameter update</U>가 지그재그로 진행되었던 기존 단점을 $\tanh$는 해결할 수 있다. 하지만 이 경우에 대해서도 너무 크거나 작은 input value에 대해 gradient saturation 문제가 발생, 신경망 학습 시 activation function 이전의 node들에 대해 <U>학습이 죽어버리는 현상</U>이 여전히 발생한다.

## Rectified Linear Unit(ReLU) function

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213348547-512ac25f-04e3-4ec0-9f2a-ce356d4f3e08.png" width="400"/>
</p>

아마도 대부분 딥러닝을 접한 사람들이 많이 들어봤을 함수이다. 형태는 정말 간단하게 threshold가 $0$인 hinge loss가 된다. 해당 activation function은 [AlexNet 논문](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)에서 확인해볼 수 있으며, 아래 그래프와 같이 sigmoid function을 사용한 학습보다 ReLU를 사용한 학습 과정이 수렴 속도가 더 빠른 것을 확인할 수 있다(그래프 상에서 실선이 ReLU를 사용한 neural network, 점선이 sigmoid를 사용한 neural network의 수렴 과정을 보여준다).

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213348888-063ecd09-a49d-4896-a950-e4c812d75937.png" width="400"/>
</p>

​ReLU 함수의 장단점은 다음과 같다.   
우선 gradient vanishing 문제가 앞선 두 경우(시그모이드, 하이퍼볼릭 탄젠트) 둘 다 존재했었는데, 이 함수의 경우 $x$가 아무리 커지더라도 gradient가 $1$로 유지된다. 또한 시그모이드, 하이퍼볼릭 탄젠트 둘 다 exponential값을 연산해야하기 때문에 computational한 점이 문제가 되었는데, ReLU function의 경우 단순히 0보다 큰 값만 내보내는 과정을 통해 연산을 단순화할 수 있다.
하지만 ReLU는 <U>'die'에 취약</U>하다. ReLU gate를 통해 <U>큰 gradient가 흘러서</U> weight가 업데이트될 때, 다시는 activate될 수 없는 부분으로 학습이 될 수도 있다. 물론 ReLU 함수의 특성 상 weight 상에서 불필요한 부분들에 대한 연산을 배제하는 효과가 있기 때문에 어느 정도 deactivation하는 과정이 필요하지만, 만약 weight parameter가 잘못된 방향으로 학습되었을 경우, 해당 위치의 parameter에 대해서 network의 모든 노드들을 <U>최적화할 수 없다는 문제</U>가 발생한다. 따라서 learning rate, weight parameter 초기화 등등이 학습에 중요하게 작용하게 되므로 robust한 학습이 어렵다는 단점이 있다.

## Leaky ReLU, Parametric ReLU

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213350026-f11acb39-a9ef-465a-b579-30e4219ac8a8.png" width="400"/>
</p>

따라서 0보다 작은 value에 대해 작은 기울기를 가지는 선형 함수를 매핑함 LeakyReLU나, 네트워크 학습 과정에서 적절한 기울기 $a$를 학습할 수 있게끔 하는($y = ax$) parameteric ReLU 등등 기존 ReLU를 대체할 수 있는 activation function들이 제시되었다. 이후 여러 연구들에서 periodic activation function, GeLU, SiLU 등등 task에 따른 적절한 activation function이 제시되고 있다.

---

# Neural network architectures

## Graph structure and the number of layers

신경망 구조는 흔히 각 뉴런들에 대해 신호가 한쪽 방향으로 흘러가는 단일 방향의 그래프로 모델링된다. 이를 간단하게 표현하자면 특정 뉴런의 output이 다른 뉴런의 input이 되고, 이 <U>순서는 서로 바뀔 수 없다</U>(not commutable). 이런 형태의 그래프에서 cyclic 구조(어떤 노드의 output이 돌고 돌아 다시 자신의 input이 되는 형태의 그래프)는 허용하지 않는데, 이는 IIR(infinite impulse response) system이기 때문이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213358377-e65cd2e1-18c0-48f2-a65f-d72420066dee.png" width="600"/>
</p>

위와 같은 그림을 가지는 neural network structure를 표현할 때 '$3$개의 layer를 가지는 network'라고 표현한다. 보통 $N$개의 layer를 가지는 neural network를 정의하는 과정에서 <U>input layer</U>는 따로 counting하지 않는 것을 볼 수 있는데, 이는 input layer는 특정 modality를 가지는 입력 신호(이미지, 음성, 텍스트) 자체를 의미하기 때문이다.   
따라서 single layer neural network의 경우에는 hidden layer가 하나도 없는 구조를 지칭하며, logistic regression이나 SVM 등등을 single layer neural network라 부른다. 물론 이전에 언급했던 linear classification도 single neural network에 해당된다. 이러한 네트워크 구조들을 ANN(Artificial Neural Networks) 혹은 MLP(Multi-Layer Perceptrons)라 지칭한다. 하나의 레이어를 담당하는 perceptron이 연결되어 깊이를 가지는 신경망 층을 생성하게 된다. 다만 output layer에는 <U>activation function이 없는 경우</U>가 있는데, 보통 <U>classification의 기준</U>이 되는 score로 사용되거나, <U>정규화 없이 regression을 진행</U>하는 task인 경우가 이에 해당된다.

## Neural network size
Neural network의 크기는 흔히 parameter의 수로 정의하기도 한다. 신경망의 layer 갯수와 차원 갯수에 따라 학습 가능한 parameter의 수가 결정되는데, 다음과 같다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213358377-e65cd2e1-18c0-48f2-a65f-d72420066dee.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213359552-f778f00f-1e94-4629-9d39-a1dcd92bc846.png" width="300"/>
</p>

그림을 보게 되면 layer가 노드(원)으로 표시가 되어있지만, 사실 실질적으로 layer 연산이 적용되는 부분은 <U>노드를 연결하는 엣지</U>(화살표)에 해당된다. 예를 들어 좌측의 구조를 가지는 네트워크에 대해 parameter 개수를 구하게 되면 bias를 포함한다고 가정했을 때,

\[
    \begin{cases}
        \text{input layer} \rightarrow \text{hidden layer 1}, & W \in \mathbb{R}^{3 \times 4},~b \in \mathbb{R}^{1 \times 4} \newline
        \text{hidden layer 1} \rightarrow \text{hidden layer 2}, & W \in \mathbb{R}^{4 \times 4},~b \in \mathbb{R}^{1 \times 4} \newline
        \text{hidden layer 2} \rightarrow \text{output layer}, & W \in \mathbb{R}^{4 \times 1},~b \in \mathbb{R}^{1 \times 1}
    \end{cases}    
\]

위와 같으므로 총 학습 가능한 parameter의 개수는 각 parameter의 차원 수를 모두 더한 $41$개가 된다. 마찬가지로 우측과 같은 구조에 대해서도 같은 방법으로 계산해보면,

\[
    \begin{cases}
        \text{input layer} \rightarrow \text{hidden layer}, & W \in \mathbb{R}^{3 \times 4},~b \in \mathbb{R}^{1 \times 4} \newline
        \text{hidden layer} \rightarrow \text{output layer}, & W \in \mathbb{R}^{4 \times 2},~b \in \mathbb{R}^{1 \times 2}
    \end{cases}    
\]

이므로 총 학습 가능한 parameter의 개수는 각 parameter의 차원 수를 모두 더한 $26$이 된다.

---

# Representation power
Neural Networks를 <U>fully connected layers</U>로 보는 방법은 네트워크를 구성하는 weight들을 parameter로 생각하여 함수를 정의하는 것이다. 이런 함수 형태의 묶음에 대해서 <U>representational power</U>를 정의할 수 있으며, Neural Network와 같이 모델링하는 방법에 대해 알아보도록 하자. 적어도 하나 이상의 hidden layer가 있는 neural network를 MLP로 정의하였고, 이때 neural network가 가지는 structural scale은 <U>적어도 2개 이상의 layer 수</U>를 가지게 된다. 이러한 neural network는 universal approximator임을 알 수 있다([참고 링크](http://neuralnetworksanddeeplearning.com/chap4.html)). Universal approximator에 대해 간략하게 설명하자면, 예를 들어 다음과 같이 real world에서 묘사하고 싶은 복잡한 함수 형태에 대해서,

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213361934-e6fce9aa-2536-4cca-aca6-50053a7d013f.png" width="400"/>
</p>

Neural network로 하여금 특정 input $x$에 대한 함숫값 $f(x)$를 예측하고자 한다. 지금은 단순히 차원 수가 $1$인(scalar인) 경우에 대한 function estimation을 가정했지만, $n$차원의 modality에 대한 multi-dimension function estimation 또한 가능하다. 즉 우리는 아무런 continous 함수 $f$를 가정할 수 있고 이러한 함수가 존재한다고 한다면 neural network function $g$를 이 함수에 근사시킬 수 있다는 것이다. 이렇게 된다면 neural network를 continous function으로 가정할 수 있게 된다.

\[
    \begin{aligned}
        \text{if any }f(x) \text{ is continous and some }\epsilon > 0, \newline
        \exists~g(x)~\text{s.t. }\forall x, \vert f(x) - g(x) \vert < \epsilon 
    \end{aligned}
\]

위의 참고 링크를 보게 되면 hidden layer 수가 하나인 neural network가 Universal approximator로 사용되는 예시들을 보여준다. 하지만 실제로 $2$개의 레이어를 가지는 neural network가 universal approximator라는 사실이 수학적으로는 가능하지만 실생활에서는(다차원의 복잡한 함수 형태가 존재하는 상황) 쓰이기 힘들다.

\[
    g(x) = \sum_i c_i 1(a_i < x < b_i)    
\]

예를 들면 위와 같은 함수도 universal approximator로 사용될 수 있지만 머신러닝에서 이런 함수꼴을 전혀 사용하지는 않는 것을 볼 수 있다. Neural Network가 실생활에서 잘 활용되기 위해서는 <U>데이터의 통계적 성질</U>이나 실생활에서 마주하는 형태를 <U>잘 반영할 수 있는 함수 형태</U>를 찾고자 하는 것이고, 이러한 함수 형태를 찾는 과정으로 gradient descent 알고리즘을 활용한 optimization이 필요하다. 따라서 깊은 네트워크(여러 개의 레이어가 있는 모델)이 실생활의 데이터 접근에 조금 더 잘 활용이 될 수 있는 것은 여러 레이어를 가지는 네트워크의 <U>representation power</U>(표현력)이 커지게 되기 때문이고, 결국 **representation power**란 <U>Universal approximator가 표현 가능한 함수의 가짓수, 복잡도</U>를 포괄하는 의미로 작용한다.

---

# Why deep learning?
일반적으로 3개의 layer를 갖는 Neural Network는 2개의 layer를 가지는 Neural Network에 비해 representation power가 크기 때문에 실생활의 task에 대해 더 좋은 성능을 보이지만, 그렇다고 해서 무작정 layer를 키우는 방식이 절대적으로 성능 지표를 올릴 수 있는 <U>key point</U>가 되지 않는다. 이러한 특성은 <U>Convolutional Neural Network</U>와 차이가 있는데, CNN은 깊이가 깊어질수록 recognition 성능이 좋아지는 경향성을 어느 정도 보이기 때문이다(사실 CNN에서도 네트워크 깊이가 무작정 깊어지는 것이 좋은 것은 아니다). Image는 계층적 특징 구조를 가지고(물체의 윤곽부터 시작해서 물체의 디테일한 texture까지), 여러 층의 layer가 계층 구조를 분리하여 특징을 추출하거나 인식하는 효과를 줄 수 있다는 것이다. 갑자기 다른 주제로 이야기가 빠졌는데 다시 돌아오게 되면, neural network의 layer 수를 결정하는데 있어서 적절한 타협이 필요하다는 것이다.


<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213365482-8584de43-5b15-4a21-92ae-74e9f527996b.png" width="700"/>
</p>

위와 같은 그림을 보면, 붉은색으로 표시된 데이터와 녹색으로 표시된 데이터를 이진 분류하는 task에 대해 hidden layer의 개수가 decision area(구분선)에 미치는 영향을 시각화한 것이다. 큰 neural network로 갈수록 representation power가 커지기 때문에 보다 복잡한 함수 형태를 가질 수 있지만 이는 training dataset에 overfitting되는 문제를 일으킨다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213365915-860b745d-fa49-4201-acb1-f67cae79684c.png" width="700"/>
</p>

Overfitting이라는 것은 네트워크로 하여금 <U>training data</U>의 noise까지 불필요하게 학습해버린다는 문제인데, 앞서 본 그림에서 $20$ layer classifier는 모든 dataset에 대해 구분 가능한 representation learning을 진행한 탓에 <U>red, green 영역이 분리된 영역</U>도 생겨버리게 된다. 이럴 경우 실제 test dataset에 대해서는 정확도가 오히려 떨어지는 결과를 불러온다. 이러한 문제를 줄이기 위해 레이어 수를 줄이는 방법만 있는 것은 아니다. 앞서 linear classification에서 언급했던 $L_2$ regularization 그리고 학습 과정에서 특정 node(weight)를 무시하는 dropout 등등 여러 방법들이 있다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213366471-78948448-5394-4835-b7b9-2a804dc7ac4f.png" width="700"/>
</p>

Classification에 사용되는 loss 뿐만 아니라 정규화 작업에 사용되는 <U>regularization loss</U>의 최적화 과정에서의 중요도를 $\lambda$라고 했을 때, 정규화에 대한 loss가 커질 수록 네트워크가 학습한 continous function이 점차 단순화되는 것을 알 수 있다. Regularization과 관련된 기존 글에서의 내용이 기억이 잘 안나겠지만 당시에 했던 말을 다시 해보자면, regularization 과정은 classification task에 있어서 <U>weight parameter가 고르게 분포하게끔</U> 해준다.   
보통 layer 수가 적은 모델일수록 local minima가 더 적다. Representation power가 증가할수록 표현 가능한 함수 복잡도가 커지지만 그만큼 weight에 따른 loss graph가 복잡하다. 그러나 layer 수가 적은 네트워크의 minima의 경우 학습하기는 쉽지만, 실제 global minima와 오차 범위가 크기 때문에(weight를 학습 가능한 축의 개수로 인식하면, feasible area가 줄어들기 때문) 학습 결과 성능이 좋지 않은 경우가 있다. 그러나 큰 모델일수록 상당히 많은 local minima가 존재하지만, 실제로 학습했을 때 더 좋은 결과를 보여줄 수 있다.   
작은 네트워크는 학습할 때 local minima의 깊이, 갯수가 현저히 적기 때문에 최적화는 훨씬 쉽지만 그만큼 성능 변동이 커서(혹은 최적화 성능이 최대 성능과 많이 차이가 나는 경우) 문제가 될 수 있고, 큰 네트워크는 local minima가 많아서 학습 과정에서 global minima를 찾기 힘들지만 이러한 variance를 감안하고서라도 학습 결과를 보았을 때 성능이 좋은 경우가 많다는 것이다. 이는 weight의 초기화 과정 자체가 결과에 미치는 영향이 줄어들어 일반화가 쉽다는 것이다.   
길게 설명했던 내용은 결국 overfitting을 방지, representation 학습을 하는 과정에서 <U>도대체 왜 딥러닝이 좋은 평가를 받을 수 있는지</U>에 대한 해석이었다. 단순히 레이어 수를 줄여서 학습하는 것보다, 깊은 레이어를 학습하되 regularization 과정을 통해 overfitting을 방지하는 것이 초기화 및 학습 단계에서 안정적인 학습을 보장한다는 것이다.

---

# 결론

얕은 레이어를 가진 모델은 <U>간단한 함수를 학습</U>하고 그만큼 수렴성이 높다는 장점이 있다. <U>Local minima</U>가 적으며 <U>global minima</U>에 보다 수렴하기 쉽지만, 수렴한 minima가 실제 네트워크의 <U>최대 성능에 비해 떨어질 수 있다</U>. 그러다보니 얕은 레이어의 모델은 weight 초기화에 변동이 매우 심하고, 그렇기 때문에 <U>loss function의 전체 구조</U>를 알 수 없는 실생활의 여러 데이터에 대해 <U>일반화될 수 없다는 문제</U>가 발생한다. 그렇기 때문에 레이어가 더 깊은 모델을 사용하는데, 앞서 설명했던 것과 같이 <U>representation power</U>가 커지면 그만큼 <U>overfitting</U> 위험성이 생긴다. 따라서 이를 해결하기 위해 레이어 수를 줄이는 방향이 아닌, <U>regularization을 사용</U>하게 된다. Deep neural network의 representation learning 과정에서 <U>local minima가 많고</U> 학습 시 수렴한 point가 <U>전체 함수의 global minima가 아닐 수 있지만</U>, 오히려 이런 loss들의 성능이 나쁘지 않아 실생활에서 다양하게 일반화하기 좋다.