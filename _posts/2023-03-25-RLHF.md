---
title: ChatGPT(챗지피티)가 사람같이 자연스럽게 말할 수 있는 이유는!? RLHF 학습법에 대하여
layout: post
description: ChatGPT, Chatbot
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/227689828-9e58a288-ae75-4736-8041-9fa89e72227c.png
category: deep learning
tags:
- AI
- Deep learning
- Reinforcement learning
- Human forcing
---

이 글은 huggingface의 게시글 중 <U>'Illustrating Reinforcement Learning from Human Feedback (RLHF)'</U>를 번역 및 각색하여 딥러닝을 잘 모르는 사람들도 이해할 수 있게 바꾼 내용이다. 원본 게시글 링크는 다음과 같다([참고 링크](https://huggingface.co/blog/rlhf)).

> Lambert, et al., "Illustrating Reinforcement Learning from Human Feedback (RLHF)", Hugging Face Blog, 2022.

# 지금은 AI의 호황기?
국내에서는 AI에 대한 일반인들의 관심이 <U>알파고 이후</U> 급증했다가 점차 사그라들었다. 그러면서 점점 4차 산업혁명이니 빅데이터 등등의 워딩을 온갖 플랫폼에 가져다놓기 시작했다. (솔직히 여기서 소신발언을 하자면 나는 딥러닝이나 머신러닝에 대해 제대로 공부해보지도 않은 사람들이 사업성 멘트로 인공지능을 넣는 행위를 싫어한다...)

아무튼 그러던 와중 ChatGPT가 이번에는 자연어 처리 쪽으로 다시 불을 지피면서 사람들의 이목을 끌기 시작했다.

<U>컴퓨터 사이언스</U>가 국내에서도 spotlight를 받기 시작하면서 원리에 대한 이해를 배제한 채로 사업 플랫폼을 인공지능이니 메타버스니 있어보이는 말에 맞추는 경향이 있었다. 물론 제대로 된 스타트업이나 사업체도 있었지만 대부분 기술에 대한 <U>명확한 솔루션</U>을 제공하지 못하는 경우가 허다했었다.

그럼에도 불구하고 <U>'인공지능'</U>이라는 키워드는 사람들을 고무시키기엔 충분했다. 하지만 그만큼 정작 딥러닝과는 전혀 관련이 없는 서비스들도(학부생 수준의 toy project로도 가능할 수준) 웹 상에 등장하기 시작했다. 물론 실제로 머신러닝을 적용한 서비스들은 있었지만 다양한 계층/문화의 사람들에게 범용적으로 제공되는 서비스라고 말할 순 없었던 것은 사실이다.

이번 글에서 소개하고자 하는 [ChatGPT](https://openai.com/blog/chatgpt)는 이러한 기존 인공지능 사업들과 비교했을때 시장성/실용성을 극한으로 뽑아낸 대표적인 사례라 볼 수 있다. 알 사람들은 알겠지만 Stability AI 기업에서 발표한 [Stable diffusion](https://stability.ai/blog/stable-diffusion-public-release)가 ChatGPT 이전에 사람들에게 하나의 컨텐츠로서 다가갈 수 있었던 대규모 모델 중 하나이지만, 검색이나 스토리텔링, 질의응답이 가능한 ChatGPT의 효용성/활용성과는 차이가 있다. 과연 챗지피티와 같은 LLM(대용량 언어 모델)이 구글과 같은 <U>검색 엔진에 의존</U>했던 <U>과거의 트렌드</U>를 완전히 뒤엎을 수 있을지는 앞으로 더 지켜봐야할 것이다.

---

# 들어가며...
딥러닝을 기존에 연구하던 사람들은 알겠지만 <U>자연어 처리</U>(NLP)와 관련된 language model은 최근 몇 년간 급진적인 발전을 이루었다. 단순 기계 번역에서 새로운 모델링을 제시한 [Transformer](https://arxiv.org/abs/1706.03762)(Attention is all you need)라는 논문이 딥러닝을 통한 기계 번역(Machine translation)에서 제시되면서 이를 활용한 GPT, BERT와 같은 모델들이 다양화되고 발전하기 시작했다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/213901161-30db60a8-bb44-46c8-9f42-ea2e45de38fe.png" width="400"/>
    <img src="https://user-images.githubusercontent.com/79881119/213901191-b4994362-af6a-4685-bcf0-9a9924ec3b1a.png" width="400"/>
</p>

GPT와 BERT에 대해 말하자면, GPT는 <U>문장을 '생성'</U>하는 과정에 집중한 자연어 처리 연구에 해당되고 BERT는 반대로 주어진 <U>문장의 문맥을 '이해'</U>하는 과정에 집중한 자연어 처리 연구라고 볼 수 있다. 따라서 <U>ChatGPT</U>가 기반을 둔 GPT모델은 결국 **'그럴듯한 문장을 생성하는 과정'**에 집중한 연구라고 할 수 있다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227694380-30f31f95-4ffc-4a30-bc46-768e0566d49d.png" width="600"/>
</p>

---

# 그럴듯한 문장이란?
컴퓨터 알고리즘은 수학으로 구성된다. 결국 문장을 만들어내는 과정 자체가 복잡하게 연산된 숫자들의 집합을 통해 계산하는 과정이 되는데, 그렇다면 사람이 어쩌면 합리적으로 인식할 수 있는 <U>'좋은 text'</U>를 정의하는 방식 또한 주관적이기도 하고, 문장을 생성하는 상황에 따라 달라질 수 있다는 것을 알 수 있다.

GPT는 창의성을 요구하는 <U>소설</U>을 써야할 수도, <U>수필</U>을 써야할 수도 있고 아니면 <U>논문의 absract</U>를 작성해야할 수도 있으며 심지어는 개발자에게 필요한 <U>코드 알고리즘 일부</U>를 작성해줄 수도 있다. 딥러닝에서 <U>학습의 주체</U>가 되는 심층 신경망은 수많은 파라미터(숫자의 집합)으로 구성되어있는데, 이 파라미터들이 '그럴듯한 문장을 생성하기 위해' setting되기 위해서는 <U>해당 목적성을 대표</U>할 수 있는 **목적 함수**를 설정해야한다. 이를 머신 러닝에서는 'loss function', 혹은 'objective function'을 정하는 문제가 된다.

대부분의 language model에서는 단순히 이전 문장을 참고하여 다음 단어를 예측하는 categorical function인 cross entropy와 같은 <U>trivial solution</U>을 가지는 목적 함수를 사용한다. 여기서 trivial solution이라면 '학습된 데이터를 기준으로, 이러한 문장 구조에서는 이러한 단어가 많이 나오더라'라는 식의 예측이다. 하지만 과연 이게 문맥을 이해해야하는 상황에 <U>제대로 부합할 것인지</U>는 알 수 없다. 이러한 문제들을 완화하기 위해 생성된 문장에 대한 human preferences(사람의 주관적 평가)가 어느 정도 반영된 metric(측정 지표)인 [BLEU](https://en.wikipedia.org/wiki/BLEU)나 [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))가 최적화에 활용되기 시작했다.

\[
    \begin{cases}
        \text{ROUGE}, &(w_{ref} \in S_{gen} \vert w_{ref} \in S_{ref}) / \vert S_{ref} \vert \newline
        \text{BLEU}, &(w_{gen} \in S_{ref} \vert w_{gen} \in S_{gen}) / \vert S_{gen} \vert
    \end{cases}
\]

위의 수식이 표현하는 바를 간단하게 요약하자면 'ROUGE'는 reference(기준)가 되는 문장($S_{ref}$)에 있는 단어($w_{ref}$)가 generated(생성된) 문장($S_{gen}$)에 포함된 정도를 나타낸 것이고 'BLEU'는 generated(생성된) 문장($S_{gen}$)에 있는 단어($w_{gen}$)가 reference(기준)가 되는 문장에 포함된 정도를 비율로 표현한 것이다. 따라서 해당 metric을 주관적 지표로 사용하더라도 실질적으로 <U>단어가 포함된 정도</U>를 나타낼 뿐이지, 생성된 각 단어가 적합한 문맥/위치에서 생성되었는지 판단할 수 없다는 문제가 생긴다. 최적화하는 과정에서 <U>한계점</U>이 드러난 것이다.

---

# RLHF의 두두등장

그렇다면 조금은 귀찮긴하지만, 만약 생성된 문장에 대해 <U>사람이 직접 feedback</U>을 해주고 이를 기반으로 <U>performance를 측정</U>(수치화)할 수 있는 방법이 있다면 언어 생성 모델의 파라미터 보다 <U>섬세하게 조작</U> 및 최적화 할 수 있지 않을까?라는 생각이 스멀스멀 피어오르게 된다. 바로 이러한 아이디어에서 나온 것이 ChatGPT를 학습한 방법인 <U>Reinforcement Learning from Human Feedback</U>(RLHF)이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227696698-d5dc083b-e0c1-4b6d-9ab2-abb3bc6bd255.png" width="400"/>
</p>

RLHF란 결국 사람의 feedback을 reinforcement learning(강화 학습)에서 사용하는 일종의 penalty/reward로 간주하여 최적화를 진행하겠다는 것이다. RLHF를 사용함으로 인해 LLM(Large Language Model)이 생성하는 문장이 <U>보다 사람이 내뱉은 문장에 가깝게</U> 학습이 될 수 있었다. 과연 챗지피티는 본인이 학습된 RLHF에 대해서 제대로 설명할 수 있을까?

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227696407-8e1db0e8-1a20-49a8-9516-e2ccc910198f.png" width="600"/>
</p>

아무래도 챗지피티는 <U>느낌표를 붙이는 것</U>이 어린 아이한테 설명하는 문맥상 포인트라고 생각했나보다. 내용은 말한 것과 같이 강화학습을 통해 학습하는데 보다 <U>빠르고 정확한 방향</U>으로 학습하고자 하는 것이 RLHF의 기본 방법이다. 지금까지는 간단하게 설명하기 위해 빌드업을 했지만, 지금부터는 구체적으로 학습하는 방법에 대해서 언급하도록 하겠다.

---

# RLHF : 단계별로 이해해보기
사실 본인도 reinforcement learning에 대해 제대로 이해하고 있는 상황은 아니기 때문에 글을 적고 있으면서도 ~~양심에 찔리지만~~.. hugging face에 있는 게시글 내용을 <U>보다 친절한 형식</U>으로 번역해보기 위해 작성하게 되었다. RLHF 학습법은 다음과 같이 여러 pipeline으로 구성된 training process를 가진다.

- Language Model(LM)을 사전 학습(Pre-training)하기
- Data를 획득하고 reward model을 학습시키기
- LM을 강화 학습으로 fine-tuning하기

요 각각의 단계들에 대해서 자세히 살펴보도록 하자.

### Pretraining language models
RLHF를 진행하려면 feedback을 받을 language model이 필요하다. [Language model을 학습](https://huggingface.co/blog/how-to-train)하는 것은 기존 <U>classical pretraining objective</U>로 학습한다(앞서 설명했던 Cross-entropy loss나 BLEU, ROUGE 등등 모두 가능). 예를 들어 OpenAI에서는 GPT-3 시리즈 중 가벼운 모델을 활용하여 첫 RLHF 방법으로 학습한 모델인 [InstructGPT](https://openai.com/research/instruction-following)를 만들었다. 다른 서비스인 [CLAUDE](https://www.anthropic.com/)를 런칭한 Anthropic의 경우에는 $10 \times 10^6 \sim 52 \times 10^9$의 parameter를 가지는 사전 학습된 transformer를 기준으로 RLHF에 적용하였고, DeepMind에서는 본인들의 모델인 [GoPher](https://arxiv.org/abs/2112.11446)를 사전 학습된 LM으로 사용하였다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227706651-afae5325-30f4-40a4-9c69-312578fbec5b.png" width="600"/>
</p>

이렇게 상정한 초기 모델(Initial model)을 fine-tuning할 수도 있다. 예컨데 OpenAI는 <U>사람이 직접 생성</U>한 "보다 선호되는(preferable)" prompt를 fine-tuning에 사용하였고, Anthropic에서는 RLHF에 활용할 LM의 초기 상태를 그들의 기준인 "helpful, honest, and harmless"라는 <U>기준에 대한 문맥</U>을 가이드를 해주는 방식을 사용하여 학습하였다. 두 방식 모두 일종의 '잘 정제된' 데이터셋을 사용한 것과 같으며, 일반적으로 고퀄리티의 데이터셋을 구성하는 과정 자체가 <U>cost가 높기 때문에</U> RLHF 방식을 사용하는데 있어 위와 같은 fine tuning 과정이 필수적인 것은 아니다.

어떠한 모델을 강화학습(RLHP)의 <U>starting point</U>로 사용하는 것이 가장 바람직한지 명확한 답은 정해진 것이 없다. 사실상 Large language model이 여러 기업들로부터 다양하게 학습되고 있기는 하지만 RLHF 방식을 사용하는데 있어서 design space를 구성하는 모든 네트워크 구조에 대해 성능비교를 한 연구는 아직 전무하기 때문이다. 즉 RLHF에서 '언어 모델을 강화하는 방법'은 아직 <U>구조에 따른 성능 차이</U>가 규명되지 않은 채로 설명된다고 생각하면 된다.

### Reward model training
언어 모델을 평가하기 위해서는 모델이 만들어내는 데이터를 얻어야한다. 사람이 실제로 선호하는 text(document)를 만들어내는 언어 모델을 만들기 위해서는, 사람이 주는 피드백을 모델에게 reward(문장을 잘 만든 정도에 따라 보상을 주는 방식)로 반영시킬 수 있는 reward model(RM), 이른바 <U>보상 모델이 필요</U>하다.

조금 더 구체적인 목표를 설정하자면, LM이 추출하는 연속된 텍스트를 받아서, 생성한 텍스트에 대한 <U>사람의 선호도</U>를 <U>scalar</U> reward(점수표 형태로 생각하면 된다)로 리턴할 수 있는 <U>보상 시스템</U>이나 <U>모델 구조</U>를 구성하고 싶은 것이다. 예컨데 보상 모델이 특정 텍스트를 보고 나름의 랭킹을 매겨주면, 이에 따른 scalar 값을 reward로 주는 것이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227707301-41fcb50f-6542-42c7-8908-500aed82f700.png" width="600"/>
</p>

RM의 output이 scalar가 되는 것이 RL 알고리즘에 있어 가장 필수적인 부분이라고 생각하면 된다. 간단하게 표현하자면 <U>사람의 선호도를 점수화</U>하는 기준을 잘 정하는 것이 RL 알고리즘이 RLHF(사람의 선호도를 네트워크의 강화 학습에 사용하는 방법)에 자연스럽게 녹아들게하는 주요 요소가 된다고 표현할 수 있다.

Reward modeling 결국 '텍스트'를 기준으로 ranking을 수치화하는 것이기 때문에 또다른 언어 모델을 preference data로 학습(처음부터 학습하거나, 사전 학습된 LM을 fine-tuning하는 두 경우를 생각해볼 수 있음)한다고 생각해볼 수 있다. 실제로 Anthropic에서는 사전 학습된 LM을 기준으로 PMP(preference model pretraining)이라는 특별한 fine-tuning 방법을 사용하였고, 이 방법이 sample efficient(상대적으로 적은 샘플로도 원하는 기능을 구현할 수 있음)하다고 밝혔다. 하지만 앞서 RLHF에서 <U>가장 효과적인 LM 구조에 대한 연구가 부족</U>하다고 말했던 것과 같이 여전히 이 부분<U>(RM의 효과적 학습)</U> 또한 확실한 정답은 밝혀지지 않은 상태이다.

RM(Reward model)을 학습하기 위해서는 학습된 네트워크가 특정 prompt를 받아서 생성한 텍스트 데이터셋이 필요한데(prompt-generation pair), 이때는 사전 데이터셋에서 샘플링하여 추출하게 된다. Anthropic의 경우 Amazoon Mechanical Turk의 <U>chat tool</U>로부터 획득한 prompt를 기준으로 생성하게 되고, Open AI는 본인들이 풀어놓은 GPT API를 <U>유저들이 사용</U>한 <U>prompt submission</U>을 사용했다고 한다(역시 공짜는 없음 ㄷㄷ).

이렇게 생성한 LM의 text output를 human annotator(랭킹을 매기는 인력 시장이라고 보면 된다)에 추면, 생성된 text에 나름대로 기준을 적용하여 줄세우기를 시작한다. 이런 세팅에서 '<U>굳이 줄세울 필요없이</U> 바로 텍스트를 보고 scalar value로 <U>점수를 매겨버리면</U> reward model이 굳이 필요없는 거 아님?'라고 생각할 수도 있는데 사실상 이게 어려운 부분이 있다. 만약 당신이 human annotator고, prompt에 대해 생성된 문장을 봤을때 한 문장이 다른 문장보다 '얼마나' 좋은지 측정하는 것이 굉장히 어렵다는 것을 느끼게 된다. 확연히 차이가 보일 정도로 문장 퀄리티에 차이가 있다면 물론 점수를 부여하는 것이 어렵지 않을 수 있지만, 실질적으로 점수를 주는 기준 범위(For example, $0 \sim 10$) 내에서 구분해서 점수를 주는 것이 어렵기 때문에 사람이 직접 score를 측정하는 것은 <U>calibration</U>이나 <U>noise</U> 문제가 발생한다. 따라서 직접 점수를 주는 방식이 아니라 여러 모델로부터 나온 text를 비교하는 식으로 상대적 점수표를 구성함으로써 <U>dataset에 대한 정규화를 진행</U>한다. 

Text에 대해 ranking을 주는 방식은 다양하다. 가장 효과적이었던 방법은 같은 prompt에 대해 두 개의 다른 LM의 output을 비교하는 것이다. 1대 1로 승부하여 승자와 패자를 결정하는 방식은 마치 체스 게임을 생각해볼 수 있는데, 이런 식으로 다른 LM과의 기준으로 output을 비교하는 것을 통해 각자의 상대적인 위치를 [Elo system](https://en.wikipedia.org/wiki/Elo_rating_system)으로 나타낼 수 있다. Elo system은 체스 뿐만이 아니라 대부분의 스포츠에서 사용되는 레이팅 방식이다. 간단하게 요약하자면 <U>'예측된 승률'</U>에 따라 점수 등락이 결정되는 형태가 된다. 예를 들어 <U>약자가 강자를 이겼다면</U> 그만큼의 추가 <U>보상이 주어지는 것</U>과 같다. 물론 이 방법만 존재하는 것은 아니고 여러 ranking 방식이 존재한다. 이렇게 정해진(라벨링된) ranking을 기준으로 학습에 활용될 점수표가 normalizing된다.

Reward model이 가지는 <U>내재적 한계점</U>은 다음과 같다. RLHF 시스템에서 학습하고자 하는 언어 모델의 크기를 어느 정도 Reward model도 따라가야한다는 점이다.
예컨데 OpenAI의 $175 \times 10^9$ 파라미터 크기를 가지는 LM은 $6 \times 10^9$의 파라미터 크기를 가지는 RM을 사용하고, Anthropic과 DeepMind는 LM과 RM의 크기가 서로 비례하면서 증가하는 경향을 보인다. 결국 특정 언어 모델의 성능을 효과적으로 피드백해주기 위한 reward model도 언어 모델에 필적하는 capacity를 가져야한다는 점이다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227710112-caa2fb85-638c-4e77-9995-b7637a36487b.png" width="600"/>
</p>

즉 학습하고자 하는 LM이 달라질 때마다 RM을 다시 학습시켜야 한다는 어려움이 존재하고, 이는 학습 process 단계에서 사전 학습된 RM을 여러 LM에 사용할 수 없다는 문제점과 동시에 모델 구조에 대한 엄밀한 비교(연구 방향)를 불가능하게 하는 요소가 된다. 사실 직관적으로 이해하려 해도, 더 큰 파라미터 수를 가지는 language model이 가지는 representation을 이해하기 위해서는 그만큼 더 큰 representation 수용력을 가진 RM을 사용하는 것이 바람직하다고 볼 수 있다.

이를 <U>최상위권 학생</U>과 <U>중하위권 학생</U>을 담당하는 <U>과외 선생님의 실력</U>에 빗대어 볼 수 있다. 상대적으로 중하위권 정도의 실력을 가지는 학생(적은 parameter 수를 가지는 네트워크)은 이를 평가하고 가르치는 과외 선생님의 실력이 그다지 기대되지는 않지만, 최상위권 실력을 가지는 학생(많은 parameter 수를 가지는 네트워크)을 평가하고 가르치는 과외 선생님의 실력은 뛰어나야할 것이다. 이렇듯 공부 실력/ 티칭 실력을 일종의 representation에 빗대어 생각해보면, 더욱 <U>큰 모델일수록</U> 보상 시스템/모델의 크기도 <U>함께 커져야 한다</U>는 점을 받아들일 수 있게 된다.

지금까지 <U>LM을 사전 학습하는 과정</U>과 <U>RM을 구성하는 방법</U>에 대해 알아보았고, 이제 특정 prompt에 텍스트를 생성하는 모델(LM)과, 생성한 텍스트에 대해 인간의 선호도를 scalar 값으로 매핑해주는 모델(RM)을 모두 얻을 수 있었다. 

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227710523-b2e06ced-140e-4f22-bb28-994df461cd98.png" width="600"/>
</p>

### Fine-tuning with Reinforcement Learning
<U>강화학습</U>(Reinforcement Learning)을 기반으로 Large language model을 학습하는 것은 사실 알고리즘 상으로는 불가능의 영역에 가까웠다. 미분 연산을 통해 직접 최적화가 가능한 backpropagation과는 다르게 강화학습은 agent가 주어진 상황에서 내리는 결정에 대해 '좋음'을 수치화하기 때문이다. 따라서 딥러닝에서 사용되는 <U>심층 신경망</U>을 학습하는 것은 <U>복잡한 함수</U>에 접근하는 작업이기 때문에 학습 pipeline을 설계하는 engineering 측면에서나, algorithm 측면에서나 쉽지 않은 일이었다.  

이에 대해 여러 기관에서는 RL의 <U>policy gradient algorithm</U>인 Proximal Policy Optimization(PPO) 방법을 사용([참고 링크](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/))하여 **initial LM의 복사본**(사전 학습된 언어 모델을 copy한 객체)의 파라미터 <U>일부/전체를 fine-tuning</U>하는 방법을 사용하게 된다. 또한 LM의 파라미터 수는 적게는 $10B$ 부터 $100B$ 이상에 이르기까지 heavy하기 때문에, 이를 모두 fine-tuning하는 것이 비효율적이다. 따라서 Low-Renk Adaptation(LoRA) 논문([참고 링크](https://arxiv.org/pdf/2106.09685.pdf))에서는 <U>고차원의 parameter</U>가 사실은 <U>low intrinsic dimension</U>의 <U>manifold</U>를 대표하게 된다는 연구에서 영감을 받아 연산 효율적인 fine-tuning 방법을 입증하였다. 

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227755020-bb0b3979-16f0-4ae8-ac7b-2f3a52e03e32.png" width="400"/>
</p>

예컨데 DeepMind의 Sparrow model([참고 링크](https://arxiv.org/pdf/2209.14375.pdf))는 선호되는 prompt answering/부정적인 답변에 대한 data bootstrapping을 기반으로 점차 <U>RL 프로세스의 policy를 개선</U>하는 pipeline을 구성하였다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227755264-f5cbfdfe-4c98-4a4b-aaff-e432c96a296e.png" width="600"/>
</p>

보다 자세하게 각 내용을 다루지는 않겠지만 궁금한 사람들은 링크를 타고 한번씩 읽어봐도 좋은 내용들인 것 같다.

PPO는 RL에서는 널리 사용되는 알고리즘이기도 하고 많은 가이드라인과 튜토리얼이 나와있을 정도로 이미 mature한 연구라고 볼 수 있다. 이렇듯 비교적 널리 알려진 연구이기 때문에 RLHF에 scalable한 approach를 통해 쉽게 적용할 수 있었다고 한다. RLHF에서 사용되는 RL은 결국 <U>large language model</U>(LM)을 어떻게 하면 <U>익숙한 알고리즘</U>을 기반으로 최적화할 수 있을지에 대한 설명으로 이어진다. 

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227756620-e29ff2c1-f9a3-4b10-8565-6b64a238ebef.png" width="550"/>
</p>

위의 그림을 참고하여 <U>Fine-tuning task</U>를 RL 문제로 나타내면 다음과 같다. 먼저, **policy**는 language model로, 받아들인 prompt에 대해 sequence of text(문장)을 만들어낸다. LM을 fine tuning하는 과정에서 parameter의 집합으로 표현된 네트워크가 결국 policy의 주체가 되고, 해당 네트워크의 output인 <U>sequence of text</U> 혹은 <U>text들의 probability distribution</U>을 **policy**로 생각해볼 수 있다. 그리고 이 policy의 **action space**는 LM의 vocabulary의 모든 token에 해당된다고 볼 수 있다.
**Observation space**는 결국 agent가 내린 결정에 대한 environment를 의미하는데, 사실상 RL 학습에 대한 LM이 마주하고 있는 환경이 정의될 수는 없기 때문에 단순히 input으로 들어올 수 있는 <U>다양한 token sequence의 집합</U>이라고 생각할 수 있다. 보다 정확하게 말하자면 input token sequence에 대한 분포가 observation space에 해당된다. 일반적인 RL에 비해 observation space가 가지는 크기가 상당히 큰데, 예컨데 모든 단어 조합을 기준으로 생각한다면 대략
\[
    \text{단어 수}^\text{입력 시퀀스의 토큰 길이}
\]
라고 볼 수 있다. 보다 다양한 observation이 가능하다는 것은 생성할 수 있는 text의 다양성이 증가한다는 의미가 된다. **Reward function**은 preference model(output에 대한 평가 지표)과 policy shift에 대한 constraint(업데이트 방식)로 구성된다.

Reward function은 결국 지금까지 언급한 모든 model이나 학습 pipeline을 RLHF process로 통합해주는 과정이 된다. 만약 dataset으로부터 prompt $x$가 주어졌을 때 초기 상태의 language model에 의해 $y_1$ text가, 현재 iteration을 기준으로 update된 policy에 의한 $y_2$ text가 각각 생성되었다고 생각해보자. 현재 policy에 대해 생성된 text인 $y_2$가 preference model을 통과하게 되면, 앞서 설명했던 것과 같이 "선호도"를 반영한 scalar value($r_{\theta}$)를 추출하게 된다. 해당 text는 initial model에 의한 text인 $y_1$과 비교되고, <U>둘의 차이</U>를 통해 <U>penalty를 연산</U>하게 된다. OpenAI, Anthropic 그리고 DeepMind의 대부분의 paper에서는 <U>분포 간 포함 정도</U>를 통해 <U>distribution 거리</U>를 나타내는 대표적 메트릭 중 하나인 Kullback-Leibler(KL) divergence를 scale한 값을 penalty 연산에 활용하였다. KL divergence($r_{\text{KL}}$) 연산은 연속된 token에 걸쳐 계산된 분포 사이에 계산된다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227758349-88896b92-f828-4dec-a931-c4d2ac94d7f0.png" width="700"/>
</p>

<U>KL divergence 메트릭</U>은 이 글에서 언급하고자 하는 RLHF 뿐만 아니라 분포를 다루는 task나 네트워크 구조에서 자주 사용된다. 그러한 방식들과 차이가 있다면, 대부분 분포를 최적화하고자 하는 task에서는 이상적인 분포에 encoder나 decoder의 output 혹은 feature map을 align하기 위해 <U>KL divergence를 최소화</U>하는 방향으로 학습시키지만, RLHF의 경우에는 초기 모델과 policy optimized 모델이 생성하는 텍스트가 어느 정도 적절한 간격을 두고 차이를 보여야하므로 <U>정규화 작업에 사용된다</U>는 점이다. 만약 해당 정규화 term이 없다면 policy는 단순히 preference model을 속이기 위해 <U>기상천외한 text를 생성하더라도</U> 높은 reward를 받을 수 있게 된다(위의 그림 참고). 따라서 RL update rule에 적용될 reward는 다음과 같은 규칙을 따르게 된다.

\[
    r = r_{\theta} - \gamma r_{\text{KL}}    
\]

Reward를 최대화하긴 하는데, initial LM이 추출하는 sequence of text의 분포와 너무 멀어진다면 reward를 깎아버리는 것이다.

몇몇 RLHF framework에서는 해당 reward function에 추가로 term을 추가하기도 하였다. 예컨데 OpenAI에서 RLHF를 적용한 [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf)에서는 다음과 같이 pretraining gradients를 섞어주는 방식을 사용하였다.

\[
    L(\phi) = E\_{(x, y)\sim D\_{\pi_\phi^{\text{RL}}}} (r_\theta (x, y) - \beta \log (\pi_\phi^\text{RL}(y \vert x) / \pi^\text{SFT} (y \vert x))) + \gamma E_{x \sim D_\text{pretrain}} (\log (\pi_\phi^\text{RL} (x)))
\]

물론 여기서 언급하는 pretraining이란 human annotated set에 대한 gradient를 의미한다. 이처럼 RLHF의 <U>reward function</U>은 아직 <U>많은 연구가 필요</U>한 부분이라고 할 수 있다.

최종적으로는 **update rule**에 따라 현재 data batch의 reward metric을 최대화하는 방향으로 parameter update가 진행된다. PPO는 on-policy라고 부르는데, 이는 parameter가 오직 <U>현재의 batch만을 기준</U>으로 업데이트되는 프로세스를 일컫는다. PPO는 trust region optimization이라는 알고리즘을 따르는데, 간단하게 설명하자면 gradient가 업데이트될 때 중구난방으로 된다면 learning process에 노이즈가 발생하기 때문에 의도적으로 constraints를 주어 안정적인 학습을 할 수 있게 도와준다고 보면 된다.DeepMind도 Gopher에 대해 유사한 reward setup을 사용했지만 gradient를 최적화할 때 synchronous advantage actor-critic(A2C) 방법([참고 링크](http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com))을 사용했다는 점에서 차이가 있다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227759528-b3d978c7-7ed3-44db-a51e-796b75b2d0cf.png" width="700"/>
</p>

추가로, RLHF에서 reward model과 policy를 <U>같이 업데이트하는 방식</U>도 고려해볼 수 있다. RL(Reinforcement learning) policy가 업데이트되면 user는 해당 output에 대해서 초기 모델과 비교하는 방식을 통해 ranking(줄세우기)가 가능해진다. 유저가 지속적으로 관여해야한다는 점 때문에 너무 수고스럽기도 해서 아직 이런 operation에 대해서 다루는 paper는 많지 않다.

Anthropic은 이런 optional한 학습법에 대해 Iterated Online RLHF라고 명명하였고([참고 링크](https://arxiv.org/abs/2204.05862)), 이 경우에는 각 iteration에서의 policy가 ELO ranking system의 경쟁 대상이 된다.

---

# Open-source tools for RLHF(주로 이용되는 오픈 소스)
위에서 본 내용이 사실상 RLHF에 대한 모든 설명이었고, 이 부분에서는 RLHF 학습을 위한 tool을 소개하려고 한다. RLHF를 LM에 적용한 첫번째 코드는 2019년에 tensorflow 언어를 기반으로 OpenAI에서 릴리즈한 버전이 있다([lm-human-preferences 깃허브 링크](https://github.com/openai/lm-human-preferences)).

눈물을 흘릴 Pytorch 사용자들도 쓸 수 있는 repository도 왕왕 등장했다. Transformers Reinforcement Learning([TRL](https://github.com/lvwerra/trl), [TRLX](https://github.com/CarperAI/trlx)) 그리고 Reinforcement Learning for Language models([RL4LMs](https://github.com/allenai/RL4LMs))가 대표적인 깃헙 링크이다.

TRL이 만들어진 목적은 Huggingface 생태계에 잘 서식하고 있는 사전 학습된 LM을 PPO 방식을 통해 강화학습으로 fine-tuning하기 위함이다. TRL을 fork하여 만들어진 TRLX는 LLM에 PPO/ILQL([Implicit Q-Learning](https://sea-snell.github.io/ILQL_site/)) 기반의 RLHF을 적용하기 위해 만들어진 API이다. 아직 TRLX는 parameter 수가 많은 LLM에 대해서는 적용할 수는 없지만, 앞으로 scaling up된 사전 학습 모델을 적용할 수 있게끔 업데이트될 예정이라고 한다.

RL4LMs는 보다 다양한 RL 알고리즘(PPO, NLPO, A2C 그리고 TRPO)과 reward function 및 metric을 LLM의 fine-tuning에 적용할 수 있는 서비스를 제공한다. 또한 해당 서비스의 장점은 customizing이 간편하여, trasformer 기반 구조를 가지는 어떠한 encoder/decoder 에서도 유저가 reward function을 정의하기만 한다면 사용할 수 있다는 것이다. TRLX, RL4LM 모두 활발하게 experiment에 활용되고 있기 때문에, 보다 다양한 기능이 추가된 서비스로 개편될 것으로 전망되고 있다.

---

# 마무리하며...
위에서 소개된 RLHF는 상당히 좋은 성능을 보여주면서 <U>인공지능 연구</U>에 많은 영향력을 행사하기 시작했지만, 그럼에도 불구하고 명확히 존재하는 한계점이 있다. 먼저 model의 성능이 좋아지긴 했지만 여전히 <U>harmful, factually inaccurate text</U>를 생성한다는 점이다. 예컨데 다음과 같은 멍멍사운드를 매우 빠르게 생성해낸다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/227761255-0373f966-3961-4200-bab8-7bf2473ad398.png" width="700"/>
</p>

아무래도 이러한 결점을 해결하는 것이 앞으로 LM을 발전시킬 방향이 될 것이고, RLHF가 완벽하지 않다는 사실, 앞으로 <U>ChatGPT</U>와 같은 서비스가 발전해야할 방향이 된다.

RLHF에 의한 시스템을 사용할 때 <U>사람의 preference data</U>를 얻는 과정은 training loop와 별개로 진행되기 때문에 상당히 수고스러우며, 또한 RLHF의 퍼포먼스가 가지는 upper-bound가 사람의 annotation quality에 한정될 수 밖에 없다는 점도 짚을 수 있다(사람이 생성한 text, 사람이 측정한 preference).

특정 prompt에 대해 잘 쓰여진 human-text를 생성하는 것은 쉬운 일이 아니고 돈이 드는 작업이다. 그나마 다행인 것은 reward model을 학습할 때 사용되는 data scale은 지나치게 비싸진 않다는 점이다. Image dataset의 annotation에 비해 그닥 전문성이 요구되지는 않기 때문일 수도 있다. 그럼에도 불구하고 일반적인 연구실에서 감당하기는 <U>무시할 수 없는 액수</U>를 자랑하며, 큰 기업이 아니고서는 RLHF를 연구하기 힘든 실정이다. 현재 공개된 large scale 데이터셋은 Anthropic 이외에는 없는 것으로 알고 있다. 돈을 들여서 데이터셋을 획득하더라도 생기는 또다른 문제는 <U>human annotator 끼리의 주관적 의견이 충돌</U>하는 것이며, 정해진 답이 없는 preference의 경우에는 ranking variance가 존재할 수 밖에 없는 noisy labeling이 진행된다.

결국 RLHF는 다양한 네트워크 구조 및 loss function에 대한 ablation이 진행되기 힘들고, 다양한 연구실 환경에서 searching될 수 없는 분야라는 한계가 명확하게 존재한다. 가장 주요하게 발전해야할 점은 오래된 RL optimizer 알고리즘인 PPO를 사용하는 것인데, 결국 PPO보다 다른 알고리즘이 더 효율적이라는 것을 보이기 위해서는 현재의 RLHF 연구 흐름을 가속화할 수 있는 대안이 필요하다는 점에서 <U>여러 문제가 복합적으로 얽힌 상태</U>라고 볼 수 있다. 또한 RL framework에서 environment가 reward model의 연산 결과에 해당되므로, 이에 대한 연산 cost도 피할 수 없다는 점도 또다른 한계로 볼 수 있다. 이러한 online RL의 cost를 줄이기 위해서 최근에는 <U>ILQL과 같은 offline RL 방식</U>이 최적화 도구로 연구되는 중이다. 이외에도 RL의 exploration-exploitation balance(다양성과 효율 사이의 trade-off) 등등 메커니즘이 적용되지 않았고, 이러한 강화 학습의 알고리즘들을 RLHF에 잘 적용하는 것이 LLM의 성능 향상에 어떤 영향을 미칠 수 있을지 지켜봐야할 것이다.
