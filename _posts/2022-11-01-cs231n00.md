---
title: cs231n 내용 요약 (0) - What is deep learning?
layout: post
description: Lecture summary
use_math: true
post-image: https://user-images.githubusercontent.com/79881119/210934049-762c2540-c097-4107-89ba-32bd2b8bc9f3.png
category: deep learning
tags:
- AI
- Deep learning
- cs231n
---

# Overview

인공지능이란 무엇일까? 원래도 최근에 유명해진 분야이긴 하지만 그림 그리는 AI 등등 성능이 많이 올라오면서 이쪽 분야를 공부하고자 하는 사람들이 많아진 것 같다. 흔히 듣는 인공지능에 대한 용어들 중 AI, 딥러닝, 머신러닝, 데이터 사이언스나 빅데이트 등등 혼용해서 사용되는 경우가 많다. 그러다보니 결국 deep learning이란 무엇이고, 어떤 걸 공부하는 분야인지 애매해지는 경우가 생긴다. 지금부터 작성할 글들은 스탠포드 강의인 [cs231n](https://cs231n.github.io/) 관련 블로그를 참고했으며, 사실상 거의 번역본이라고 보면 된다. 모든 저작권은 해당 홈페이지에 있으며, 본인은 이를 일종의 공부 목적/도움을 받고자 하는 불특정 다수에게 어느 정도 내가 이해한 바를 기준으로 전달하려고 작성하게 되었다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210956163-47545d18-c612-4669-a8ff-32b4dbb26dfd.png" width="700"/>
</p>

---

# What is AI?
이번에 작성하는 글은 일종의 <U>오리엔테이션</U>와 같다. 본격적으로는 다음 글부터 이론적인 내용이 나오는데, 우선 강의에서 다루고자 하는 큰 주제가 무엇인지 짚고 넘어가고 싶었다. AI는 <U>artificial intelligence</U>의 약자로, 인공적으로 생성해낸 지능이라고 말할 수 있다.   
본인은 무교라 종교적인 이야기를 하고자 하는 것은 아니지만 인간이 창조할 수 있는 불가능의 영역 중 하나가 바로 인류/혹은 그와 유사한 지성을 가진 객체라 생각한다. 왜냐하면 감정이나 이성의 영역이 우주만큼 무한하고 헤아릴 수 없는 영역이기 때문. 그럼에도 불구하고 본인은 공대생으로 이 진로를 택했고, 공부하면서 느낀 점은 마치 천문학과에서 이런저런 우주의 비밀을 풀어내는 것처럼 인간도 어쩌면 인공지능을 해결하는 것이 미지의 영역에 발을 들이는 것이라 느끼기 시작했다. ~~물론 지금은 그런 걸 신경쓰기보단 SOTA 논문 찾기에 바쁘지만~~   
아무튼 어찌저찌 다시 결론을 내자면 AI는 인간의 지성/지능을 대표하는 하나의 객체라고 말할 수 있고, 이런 객체가 <U>로봇</U>으로 나타날수도(하드웨어), <U>프로그램</U>으로 나타날수도(소프트웨어) 있다. 인간이 여러 감각 정보들을 받아들인 후 뇌에서 이를 처리해서 특정 정보로 인식하는 프로세스를 컴퓨팅 환경으로 생각하면, 특정 modality를 sensing하는 인터페이스가 있고 이를 종합적으로 처리할 수 있는 프로세싱 모듈로 하여금 정보 해석 능력을 요구하게 된다.   
따라서 우리가 흔히 가장 큰 바운더리로 언급할 수 있는 것이 AI이며, 이는 인공적으로 만든 모든 지능의 객체가 표현되는 방식이라고 볼 수 있다. 이러한 AI를 구현하는데 필요한 것이 바로 인터페이스로 하여금 받아들이는 <U>정보(information)</U>, 정보 해석 능력을 요구당하는 <U>기계의 학습(machine learning)</U>으로 구성된다. 이를 최근 들어 조금 더 세분화하여 AI 최근 분야에서는 정보 처리와 관련된 기술을 <U>data processing/data science</U>로 분류했으며, 데이터 사이언스에서 주로 포커싱하는 것은 딥러닝/머신러닝과 같이 기계가 학습되는 부분에 대한 방법론을 제시하는 것보다 정보를 잘 정제해서 유의미한 인사이트를 알고리즘에 활용하고자 하는 것이다. 그리고 머신러닝의 다양한 방법론 중 하나인 neural network(신경망) 학습이 gradient descent based optimization과 빅데이터(방대한 데이터를 의미한다), GPU와 같은 하드웨어의 발전으로 현실적인 연구가 가능해지면서 발전한 것이 딥러닝(deep learning)이고, 아마도 대부분 최근에 AI와 관련되어 들어보았던 내용은 딥러닝 base인 연구들이 많았을 것이다. 

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210958358-5a0abde7-49fc-47b1-a5aa-08dcd5084c8f.png" width="700"/>
</p>

크게 데이터와 머신러닝의 축으로 진행되던 AI에서 <U>딥러닝</U> 분야가 발전된 이유는 feature engineering 때문이다. 컴퓨터에게 데이터로 하여금 잘 정제해서 유의미한 인사이트를 전달하는 것이 필요한데 전통적인 머신러닝 방식은 데이터를 가지고 와서 이를 분류하거나 서로 다른 이미지에서 같은 물체를 찾아내는 작업에 등등에 대해 여러 정형화된 알고리즘을 활용했고, 만약 데이터셋의 feature를 제대로 가공할 수 있는 알고리즘을 찾지 못하면 좋은 성능을 기대하기 힘든 경우가 많았다. 결국 인공지능인데 input에 대해 기대하는 output을 라벨링하는 것 뿐만 아니라, input으로 들어가는 modality에서 유의미한 feature를 가공하는 작업조차 인간이 하나의 알고리즘으로 만들어줘야하고, 사실상 이렇게 만들어진 알고리즘은 <U>데이터 수가 많아질수록 일반화 성능이 떨어진다</U>는 문제가 있었다. 결국 우리는 input에서 스스로 유의미한 feature를 찾아내고 이를 활용하여 결과를 낼 수 있는 시스템을 만들고 싶었고, 바로 deep learning은 deep neural network based learning을 활용하여 기존 머신러닝의 성능을 뛰어넘은 분야가 되었다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/210959561-07549eed-d4d4-4fc4-a4ec-27163c86dc04.png" width="400"/>
</p>

간단하게 설명하면 다음과 같다. Input space를 $X$, feature space를 $Y$, 각 input $x \sim X$에 대해 대응되는 output $z \sim Z$가 있다고 가정해보자. 여기서 space는 하나의 <U>집합</U>이고, 물론 우리는 세상에 존재하는 모든 인과관계에 대한 supervision을 가질 수 없기 때문에 각 space는 열린계로 가정하되 우리가 관측 가능한 subspace에 대해서만 본다고 생각해보자. Subspace란 부분 집합이라고 생각하면 된다. 일반적인 머신러닝에서는 input을 통한 output 예측 과정이 다음과 같다. 고정된 알고리즘 $F,~G$에 대해서,

\[
    F(x) = y,~G(y) = \hat{z}\ \approx z
\]

위와 같이 합성 함수의 형태로 표현할 수 있다. 물론 실제로 함수가 존재하는 것은 아니지만 input $X$로 하여금 output $Z$를 잘 예측할 수 있는 $F,~G$를 모델링하고자 한다. 보통의 머신러닝에서는 feature extraction 알고리즘에 해당되는 $F$가 feature engineering을 통해 실현되고 update가 되지 않는다는 문제가 있다. 여기서 update가 되지 않는다는 것은 구현한 알고리즘이 추가 데이터나 학습을 통한 성능 향상을 이뤄낼 수 없다는 것이다. 이와 마찬가지로 함수 G는 feature engineering을 통해 추출한 feature extraction을 활용하여 input에 대한 추론을 시작한다. 앞서 feature engineering이 <U>성능 수렴의 문제</U>가 있었기 때문에 이 부분도 더이상 성능 향상을 기대할 수 없게 된다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211228489-a36aba3f-89db-4378-b048-7aff011d6290.png" width="600"/>
</p>

바로 이런 측면에서 흔히 머신러닝과 딥러닝의 차이를 보여줄 때 등장하는 그래프가 나오게 된다. 데이터 수가 많아질수록 학습할 수 있는 resource는 많아지는데, 전통적인 알고리즘 방식으로는 성능 향상을 기대하기 힘들었기 때문에 딥러닝을 사용하여 기존 방식보다 성능을 높여보겠다는 시도로 인공지능의 발전을 이끌어낼 수 있었다.

<p align="center">
    <img src="https://user-images.githubusercontent.com/79881119/211228640-ce8a50f8-64b5-433b-a7c7-6dcf858502f7.png" width="400"/>
</p>

최대한 전반적인 부분을 다루고자 장황하게 설명한 것 같지만 위의 다이어그램과 같이 포함된다고 생각하면 된다. **AI**라는 큰 concept에서 소프트웨어나 하드웨어의 퍼포먼스를 구현하기 위해 제시된 **머신러닝**이 있고, 이러한 머신러닝의 여러 알고리즘 중 NN(Neural Network)를 발전시킨게 **딥러닝**이다.

---

# 마무리하면서...
간단하게 인공지능에 대해서만 언급하고 이번 글은 마친다. cs231n은 인공지능 공부를 처음 시작하면서 수강했던 강의기도 하고, 사실 지금 와서도 굉장히 스탠다드한 입문 강의로 많이 추천하기는 하지만 정말로 인공지능 입문에 적합한지는 사실 아직은 잘 모르겠다. 항상 공부하면서 느끼는 건데 이 분야는 정답이 정해진 루트가 있는게 아니라 그냥 내가 열심히 해야만 무언갈 얻을 수 있는 것 같다. 갑자기 일기장이 된 것 같지만 암튼 마무리